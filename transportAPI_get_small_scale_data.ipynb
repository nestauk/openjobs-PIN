{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransportAPI script: data collection\n",
    "\n",
    "TODO: description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant folders\n",
    "folder1= '/Users/stefgarasto/Local-Data/'\n",
    "folder2 = '/Users/stefgarasto/Google Drive/Documents/data/'\n",
    "folder3 = '/Users/stefgarasto/Google Drive/Documents/results/'\n",
    "folder4 = '/Users/stefgarasto/Google Drive/Documents/data/ONS/derivative-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant files\n",
    "ons_pc_file = folder2 + 'ONS/ONS-Postcode-Directory-Latest-Centroids.csv'\n",
    "pop_density_file = folder2 + 'ONS/population_density1.csv'\n",
    "ttwa_file = folder2 + 'ONS/Travel_to_Work_Areas_December_2011_Boundaries.csv'\n",
    "bres_lsoa_file = folder2 + 'ONS/BRES_employment_lsoa_2011_total.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, load the list of all TTWA\n",
    "ttwa_data = pd.read_csv(ttwa_file)\n",
    "# first column is ttwa codes, second column is ttwa names\n",
    "print(ttwa_data[ttwa_data['ttwa11nm'].map(lambda x: x[0:1]) == 'S'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the extracted dictionaries of OA centroids\n",
    "loadOA = True\n",
    "loadLSOA = True\n",
    "oa_centroid_path = folder4 + 'oa_centroids_dictionary.pickle'\n",
    "lsoa_centroid_path = folder4 + 'lsoa_centroids_dictionary.pickle'\n",
    "exists = os.path.isfile(oa_centroid_path)\n",
    "if exists and loadOA:\n",
    "    print('Loading the OA data')\n",
    "    oa_data = pd.read_pickle(oa_centroid_path)\n",
    "else:\n",
    "    print('File not found or not requested')\n",
    "exists = os.path.isfile(lsoa_centroid_path)\n",
    "if exists and loadLSOA:\n",
    "    print('Loading the LSOA data')\n",
    "    lsoa_data = pd.read_pickle(lsoa_centroid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# TODO just once: augment the LSOA dataset with a list of OAs in that LSOA\n",
    "tmp = []\n",
    "for ii,lsoa in enumerate(lsoa_data.index):\n",
    "    # get which OAs are in this LSOA\n",
    "    all_oas = oa_data[oa_data['lsoa11']==lsoa].index\n",
    "    tmp.append([list(all_oas)])\n",
    "lsoa_tmp = pd.DataFrame(tmp, index = lsoa_data.index, columns = ['oa_list'])\n",
    "# join with stafford_lsoa\n",
    "lsoa_data = lsoa_data.join(lsoa_tmp)\n",
    "lsoa_data.to_pickle(lsoa_centroid_path)\n",
    "'''\n",
    "print('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsoa_data.head())\n",
    "print(oa_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, I will focus on TTWA Stafford, with code E30000271, which has 95 LSOAs\n",
    "- After getting all the OAs in that area, I will get the job breakdown for each OA\n",
    "\n",
    "- Then I sum the number of jobs across all OAs in the same LSOA\n",
    "- Then I order the LSOAs by number of jobs, in descending order\n",
    "- Starting from the LSOA with the most jobs, I call the transport API to compute journey time from each other LSOA to one destination LSOA. The latter is selected going down the list of LSOA ordered by number of jobs, until I  finish the free calls available this month with transport API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the OAs and LSOAs in the Stafford TTWA and call the lmi for all to get the job breakdown\n",
    "t0 = time.time()\n",
    "stafford_ttwa = 'E30000262'\n",
    "#'E30000262' is for Shrewsbury\n",
    "#'E30000228' is for Leamington Spa\n",
    "#'E30000209' # Gloucester\n",
    "#'E30000194' Corby\n",
    "#'E30000189' Cheltenham\n",
    "#'E30000271' Stafford\n",
    "stafford_oa = oa_data[oa_data['ttwa'] == stafford_ttwa]\n",
    "stafford_lsoa = lsoa_data[lsoa_data['ttwa'] == stafford_ttwa]\n",
    "ttwa_name = ttwa_data['ttwa11nm'][ttwa_data['ttwa11cd']==stafford_ttwa].values[0].lower().replace(' ','-') \n",
    "#'cheltenham'\n",
    "print('There are {} OAs in the {} TTWA'.format(len(stafford_oa), ttwa_name))\n",
    "print('There are {} LSOAs in the {} TTWA'.format(len(stafford_lsoa), ttwa_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call lmi for all for the job breakdown or load from memory\n",
    "stafford_oa_path = folder4 + '{}_oa_lsoa_jobs.pickle'.format(ttwa_name)\n",
    "t0 = time.time()\n",
    "exists = os.path.exists(stafford_oa_path)\n",
    "if exists:\n",
    "    with open(stafford_oa_path, 'rb') as f:\n",
    "        stafford_lsoa_ordered,stafford_oa_number_of_jobs,stafford_jobs_socGroups,\\\n",
    "            stafford_oa_jobs_breakdown,stafford_oa_population = pickle.load(f)\n",
    "else:\n",
    "    stafford_oa_number_of_jobs = {}\n",
    "    stafford_oa_jobs_breakdown = {}\n",
    "    stafford_jobs_socGroups = {}\n",
    "    stafford_oa_population = {}\n",
    "    for ii,oa in enumerate(stafford_oa.index):\n",
    "        urlname = 'http://api.lmiforall.org.uk/api/v1/census/jobs_breakdown?area={:6f}%2C{:6f}'.format(\n",
    "            stafford_oa.loc[oa]['lat'],stafford_oa.loc[oa]['long'])\n",
    "        out = requests.get(urlname).json()\n",
    "        stafford_oa_number_of_jobs[oa] = out['totalJobs']\n",
    "        out = out['jobsBreakdown']\n",
    "        tmp = {}\n",
    "        for itmp in out:\n",
    "            # use the socGroup as the key (adding value or pecentage), so that then each SOC will become a column\n",
    "            tmp[itmp['socGroup']+'_value'] = itmp['value']\n",
    "            tmp[itmp['socGroup']+'_percentage'] = itmp['percentage']\n",
    "            # at the same time, keep a list of names associated with socgroups\n",
    "            stafford_jobs_socGroups[itmp['socGroup']] = itmp['description']\n",
    "        stafford_oa_jobs_breakdown[oa] = tmp\n",
    "        # get also the number of residents (population based estimate)\n",
    "        urlname = 'http://api.lmiforall.org.uk/api/v1/census/resident_occupations?area={:6f}%2C{:6f}'.format(\n",
    "            stafford_oa.loc[oa]['lat'],stafford_oa.loc[oa]['long'])\n",
    "        out = requests.get(urlname).json()\n",
    "        stafford_oa_population[oa] = out['totalResidents']\n",
    "\n",
    "print('Done, in {:4f}s'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompute the population weighted centroids and the jobs weighted centroids\n",
    "pop_lats = []\n",
    "pop_longs = []\n",
    "jobs_lats = []\n",
    "jobs_longs = []\n",
    "for ii,lsoa in enumerate(stafford_lsoa.index):\n",
    "    oa_list = stafford_lsoa['oa_list'].loc[lsoa]\n",
    "    tmp = []\n",
    "    tmp_jobs = []\n",
    "    tmp_lat = []\n",
    "    tmp_long = []\n",
    "    for oa in oa_list:\n",
    "        tmp.append(stafford_oa_population[oa])\n",
    "        tmp_jobs.append(stafford_oa_number_of_jobs[oa])\n",
    "        tmp_lat.append(stafford_oa['lat'].loc[oa])\n",
    "        tmp_long.append(stafford_oa['long'].loc[oa])\n",
    "    # transform into numpy array and normalise into proportions (so that it sums to 1)\n",
    "    tmp = np.array(tmp)/sum(tmp)\n",
    "    tmp_jobs = np.array(tmp_jobs)/sum(tmp_jobs)\n",
    "    tmp_lat = np.array(tmp_lat)\n",
    "    tmp_long = np.array(tmp_long)\n",
    "    pop_lats = np.around(np.sum(tmp_lat * tmp), decimals = 5)\n",
    "    pop_longs = np.around(np.sum(tmp_long * tmp), decimals = 5)\n",
    "    jobs_lats = np.around(np.sum(tmp_lat * tmp_jobs), decimals = 5)\n",
    "    jobs_longs = np.around(np.sum(tmp_long * tmp_jobs), decimals = 5)\n",
    "\n",
    "# now add the columns to the main dataframe\n",
    "stafford_lsoa_aug0 = stafford_lsoa.join(pd.DataFrame({'pop_lat': pop_lats, 'pop_long': pop_longs,\n",
    "                                                     'jobs_lat': jobs_lats, 'jobs_long': jobs_longs},\n",
    "                                                 index = stafford_lsoa.index, \n",
    "                                                    columns = ['pop_lat', 'pop_long','jobs_lat','jobs_long']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the number of jobs across all OAs in the same LSOA\n",
    "stafford_lsoa_number_of_jobs = []\n",
    "stafford_lsoa_density_of_jobs = []\n",
    "stafford_lsoa_max_of_jobs = []\n",
    "for ii,lsoa in enumerate(stafford_lsoa.index):\n",
    "    oa_list = stafford_lsoa['oa_list'].loc[lsoa]\n",
    "    tot_lsoa_jobs = []\n",
    "    for oa in oa_list:\n",
    "        tot_lsoa_jobs.append(stafford_oa_number_of_jobs[oa])\n",
    "    # add the absolute number of jobs\n",
    "    stafford_lsoa_number_of_jobs.append(sum(tot_lsoa_jobs))\n",
    "    stafford_lsoa_density_of_jobs.append(np.mean(tot_lsoa_jobs))\n",
    "    stafford_lsoa_max_of_jobs.append(max(tot_lsoa_jobs))\n",
    "# augment the dataframe with the total number of jobs\n",
    "stafford_lsoa_aug1 = stafford_lsoa_aug0.join(pd.DataFrame({'number_of_jobs': stafford_lsoa_number_of_jobs, \n",
    "                                                 'density_of_jobs': stafford_lsoa_density_of_jobs,\n",
    "                                                 'max_of_jobs': stafford_lsoa_max_of_jobs},\n",
    "                                                 index = stafford_lsoa.index, \n",
    "                                                    columns = ['number_of_jobs','density_of_jobs','max_of_jobs']))\n",
    "\n",
    "print(stafford_lsoa_aug1.head(n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now augment with number of jobs from BRES too\n",
    "# compare the numbers that you got like this with the number of jobs provided by the BRES employment data per LSOA\n",
    "bres_lsoa_data = pd.read_csv(bres_lsoa_file)#, error_bad_lines = False)\n",
    "# drop missing values\n",
    "bres_lsoa_data = bres_lsoa_data.dropna() # = bres_lsoa_data[not np.isnan(bres_lsoa_data['Employment'])]\n",
    "# remove last two lines, they are irrelevant information:\n",
    "#N = len(bres_lsoa_data)-1\n",
    "#bres_lsoa_data = bres_lsoa_data.drop(bres_lsoa_data.index[N])\n",
    "#N = len(bres_lsoa_data)-1\n",
    "#bres_lsoa_data = bres_lsoa_data.drop(bres_lsoa_data.index[N])\n",
    "\n",
    "# load the employments figures as defined in the ONS Business Register Employment Survey\n",
    "'''\n",
    "bres_lsoa_data['2011 super output area - lower layer'].iloc[0][:9]\n",
    "\n",
    "# extract the lsoa codes\n",
    "def extract_code(long_code):\n",
    "    try:\n",
    "        output = long_code[:9]\n",
    "    except:\n",
    "        output = 'none'\n",
    "    return output\n",
    "bres_lsoa_data['lsoa'] = bres_lsoa_data['2011 super output area - lower layer']\n",
    "bres_lsoa_data['lsoa'] = bres_lsoa_data['lsoa'].apply(extract_code)\n",
    "# remove the rows that are 'none'\n",
    "bres_lsoa_data = bres_lsoa_data[bres_lsoa_data['lsoa']!='none']\n",
    "'''\n",
    "# set the index as the LSOA code\n",
    "bres_lsoa_data = bres_lsoa_data.set_index('mnemonic')\n",
    "\n",
    "# get the number of jobs for Staffor LSOAs \n",
    "new_jobs = []\n",
    "for ii,lsoa in enumerate(stafford_lsoa_aug1.index):\n",
    "    new_jobs.append(int(bres_lsoa_data['Employment'].loc[lsoa]))\n",
    "#print(stafford_lsoa_ordered.index[0])\n",
    "#print(new_jobs[0], stafford_lsoa_ordered['number_of_jobs'].iloc[0])\n",
    "\n",
    "stafford_lsoa_aug = stafford_lsoa_aug1.join(pd.DataFrame({'number_of_jobs_BRES': new_jobs},\n",
    "                                                 index = stafford_lsoa.index, \n",
    "                                                    columns = ['number_of_jobs_BRES']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order the LSOAs by number of jobs, in descending order\n",
    "#stafford_lsoa_ordered2 = stafford_lsoa_aug.sort_values('number_of_jobs', ascending = False)\n",
    "stafford_lsoa_ordered = stafford_lsoa_aug.sort_values('number_of_jobs', ascending = False)\n",
    "\n",
    "# plot the number of jobs, just to see if there is a knee somewhere\n",
    "plt.figure(figsize = (20,5))\n",
    "tmp = plt.plot(stafford_lsoa_ordered['number_of_jobs'],'-x')\n",
    "tmp = plt.plot(stafford_lsoa_ordered['density_of_jobs'],'-o')\n",
    "tmp = plt.plot(stafford_lsoa_ordered['max_of_jobs'],'-s')\n",
    "tmp = plt.plot(stafford_lsoa_ordered['number_of_jobs_BRES'],'-d')\n",
    "plt.xlabel('LSOA in {}'.format(ttwa_name) , fontsize = 12)\n",
    "plt.ylabel('Jobs', fontsize = 12)\n",
    "tmp =plt.xticks(stafford_lsoa_ordered.index, rotation = 'vertical', size = 11)\n",
    "ax = plt.gca()\n",
    "for item in (ax.get_yticklabels()):\n",
    "    item.set_fontsize(11)\n",
    "plt.legend(['Absolute job number', 'job density', 'max jobs in one OA', 'Absolute job number from BRES'], \n",
    "           fontsize = 12)\n",
    "#plt.ylabel()\n",
    "SAVEFIG = False\n",
    "if SAVEFIG:\n",
    "    plt.savefig(folder3 + 'PIN/{}_jobs_per_LSOA.svg'.format(ttwa_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the list, with the number of jobs, and the data collected before (jobs number and breakdown per OA)\n",
    "SAVE_LSOA_DATA = False\n",
    "if SAVE_LSOA_DATA:\n",
    "    with open(stafford_oa_path, 'wb') as f:\n",
    "        pickle.dump((stafford_lsoa_ordered,stafford_oa_number_of_jobs,stafford_jobs_socGroups,\n",
    "                     stafford_oa_jobs_breakdown,stafford_oa_population),f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the two values of number of jobs for the Stafford TTWA\n",
    "np.array(new_jobs).shape\n",
    "plt.plot(np.array(new_jobs), stafford_lsoa_ordered['number_of_jobs'],'o')\n",
    "plt.xlabel('From BRES')\n",
    "plt.ylabel('From Census')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the 10 OAs with the most jobs\n",
    "origin_lsoas= list(stafford_lsoa_ordered.index)\n",
    "N = int(np.ceil(len(origin_lsoas) / 10))\n",
    "#N = int(np.around(92 / 10))\n",
    "print(N)\n",
    "dest_lsoas = list(stafford_lsoa_ordered.index[0:N])\n",
    "stafford_lsoa_ordered.index[0:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here on, it's about the travel time matrix\n",
    "# define my app_key and app_id\n",
    "SETUPKEY = True\n",
    "if SETUPKEY:\n",
    "    app_key = '6d207ab55f2768d85de4124b5fc4844c'\n",
    "    app_id = '87edbe5c'\n",
    "    app_key_jyl = '09c50d6b59698d5cbe85b50ee758baf6'\n",
    "    app_id_jyl = 'c99a83a0'\n",
    "    lon_from= '{}'\n",
    "    lat_from = '{}'\n",
    "    lon_to = '{}'\n",
    "    lat_to = '{}'\n",
    "    dep_date = '2019-06-27' #set the departure date to the next working day\n",
    "    dep_time = '07:30' # set the departure time to 7am\n",
    "    # this is the base urlname to call\n",
    "    MY_KEY = True\n",
    "    urlname_var = 'https://transportapi.com/v3/uk/public/journey/from/lonlat:{:5f},{:5f}/to/lonlat:{:5f},{:5f}/'\n",
    "    if MY_KEY:\n",
    "        urlname_fix = 'at/{}/{}.json?app_id={}&app_key={}'.format(\n",
    "            dep_date,dep_time,app_id,app_key)\n",
    "    else:\n",
    "        urlname_fix = 'at/{}/{}.json?app_id={}&app_key={}'.format(\n",
    "            dep_date,dep_time,app_id_jyl,app_key_jyl)\n",
    "\n",
    "    print(urlname_var + urlname_fix)\n",
    "    nb_of_calls = 0\n",
    "    max_calls = 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select destination LSOA\n",
    "# CAREFUL: This will call the TransportAPI\n",
    "# NOTE: I need to get one more for Chelthenham and I need to check if I missed anything for Corby\n",
    "API = True #TODO: Need to finish up the missing LSOA (missing because of the limit of 100 calls per day)\n",
    "t0 =time.time()\n",
    "if API:\n",
    "    lsoa_destination1 = 'E01028973' #'E01028945' #'E01028885' #'E01028877' \n",
    "    #'E01028873' #'E01028883' #'E01028962' #'E01028953' #'E01028941'  #'E01028942'\n",
    "    \n",
    "    '''\n",
    "    This if for Gloucester\n",
    "    #'E01022455' 'E01022247' #'E01022347' #'E01022377' #'E01022403' #'E01022318' #'E01022376' \n",
    "    #'E01022287' #'E01022277' #'E01022410' \n",
    "    #'E01022276' #'E01022348' #'E01022282' #'E01022412' #'E01022446' #'E01022341' #'E01022312'\n",
    "    '''\n",
    "    '''\n",
    "    This is for Corby\n",
    "    lsoa_destination1 = 'E01026973' #'E01026977' #'E01026962' #'E01026984' #'E01026982'\n",
    "    '''\n",
    "    '''\n",
    "    This is for Chelthenham\n",
    "     #'E01022420' #'E01022435' #'E01022454' #'E01022436' #'E01022116' #'E01022432' #'E01022106' \n",
    "    #'E01022107' #'E01022147' #'E01022123', 'E01022131'\n",
    "    TODO: for Cheltenham I still need to get some missing LSOA for E01022131 and all lsoa for the 12th destination\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    # This is for Stafford\n",
    "    lsoa_destination1 = 'E01029723' #'E01029742' #'E01029712' \n",
    "    #'E01029734' #'E01029732' #'E01029699' #'E01029744' #'E01029722' #'E01029725' #'E01029693'\n",
    "    '''\n",
    "    save_path_dest1 = folder3 + 'PIN/{}_all_lsoa_commute_{}.pickle'.format(ttwa_name,lsoa_destination1)\n",
    "    all_commute_times = {lsoa_destination1: {}}\n",
    "    for ii,lsoa in enumerate(stafford_lsoa_ordered.index):\n",
    "        if lsoa == lsoa_destination1:\n",
    "            all_commute_times[lsoa_destination1][lsoa] = {'routes': []}\n",
    "        # the API returns a json object, reading and decoding the json object returns a dict\n",
    "        urlname = urlname_var.format(stafford_lsoa_ordered['long'].loc[lsoa],\n",
    "                                 stafford_lsoa_ordered['lat'].loc[lsoa],\n",
    "                                 stafford_lsoa_ordered['long'].loc[lsoa_destination1],\n",
    "                                 stafford_lsoa_ordered['lat'].loc[lsoa_destination1]) + urlname_fix\n",
    "        out = requests.get(urlname).json()\n",
    "        nb_of_calls += 1\n",
    "        all_commute_times[lsoa_destination1][lsoa] = out\n",
    "        # save at every iteration\n",
    "        with open(save_path_dest1, 'wb') as f:\n",
    "            pickle.dump(all_commute_times, f)\n",
    "        print('Got to iteration {} for origin LSOA {}'.format(ii,lsoa))\n",
    "        if nb_of_calls==max_calls:\n",
    "            break\n",
    "print('Time spent on {} calls was {:.4f}s'.format(ii+1,time.time()-t0))\n",
    "print(nb_of_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this version is to fill in missing data in already existing files\n",
    "API = True\n",
    "ADJUST = True\n",
    "if ADJUST:\n",
    "    # load all saved data, check what's missing\n",
    "    version_load = ''\n",
    "    for dest_lsoa in dest_lsoas:\n",
    "        lsoa_commute_file = folder3 + 'PIN/{}_all_lsoa_commute_{}{}.pickle'.format(\n",
    "            ttwa_name,dest_lsoa,version_load)\n",
    "        with open(lsoa_commute_file, 'rb') as f:\n",
    "            lsoa_commute = pickle.load(f)\n",
    "\n",
    "        # get the missing origin lsoas\n",
    "        missing_lsoas = []\n",
    "        for ii,lsoa in enumerate(origin_lsoas):\n",
    "            # check whether this origin is missing\n",
    "            lsoanotin = not lsoa in lsoa_commute[dest_lsoa]\n",
    "            if not lsoanotin:\n",
    "                # check whether the 'routes' field is missing\n",
    "                routesnotin = not 'routes' in lsoa_commute[dest_lsoa][lsoa]\n",
    "                # finally, check whether 'routes' is iempty:\n",
    "                if (not routesnotin) and (dest_lsoa!=lsoa):\n",
    "                    isroutesempty = not len(lsoa_commute[dest_lsoa][lsoa]['routes'])\n",
    "                else:\n",
    "                    isroutesempty = False\n",
    "            else:\n",
    "                routesnotin = False\n",
    "            if lsoanotin or routesnotin or isroutesempty:\n",
    "                missing_lsoas.append(lsoa)\n",
    "        flag_api = len(missing_lsoas) and nb_of_calls<max_calls #(len(missing_lsoas)+nb_of_calls)<= max_calls\n",
    "        print(len(missing_lsoas), flag_api==True)\n",
    "        #continue\n",
    "        if flag_api and API:\n",
    "            print('Number of LSOA to process for destination LSOA {} is {}'.format(dest_lsoa,len(missing_lsoas)))\n",
    "            #all_commute_times = {lsoa_destination1: {}}\n",
    "            for ii,lsoa in enumerate(missing_lsoas):\n",
    "                # the API returns a json object, reading and decoding the json object returns a dict\n",
    "                urlname = urlname_var.format(stafford_lsoa_ordered['long'].loc[lsoa],\n",
    "                                         stafford_lsoa_ordered['lat'].loc[lsoa],\n",
    "                                         stafford_lsoa_ordered['long'].loc[dest_lsoa],\n",
    "                                         stafford_lsoa_ordered['lat'].loc[dest_lsoa]) + urlname_fix\n",
    "                out = requests.get(urlname).json()\n",
    "                nb_of_calls += 1\n",
    "                lsoa_commute[dest_lsoa][lsoa] = out\n",
    "                # save at every iteration\n",
    "                with open(lsoa_commute_file, 'wb') as f:\n",
    "                    pickle.dump(lsoa_commute, f)\n",
    "                print('Got to iteration {} for destination LSOA {} and origin LSOA {}'.format(ii,dest_lsoa,lsoa))\n",
    "                if nb_of_calls==max_calls:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Maps API Key:\n",
    "AIzaSyD1c5PVpgYpSh4TMLCrZ51lAiWTh3Bdg-M\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
