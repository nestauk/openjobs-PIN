{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed for propeR\n",
    "from rpy2.robjects import pandas2ri, numpy2ri\n",
    "pandas2ri.activate()\n",
    "numpy2ri.activate()\n",
    "from rpy2.robjects.packages import importr\n",
    "propeR = importr('propeR')\n",
    "\n",
    "# general imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "from importlib import reload\n",
    "import random\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# transport\n",
    "import requests\n",
    "import geopy.distance\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_1 = (52.2296756, 21.0122287)\n",
    "coords_2 = (52.406374, 16.9251681)\n",
    "t0 = time.time()\n",
    "for t in range(2):\n",
    "    print(geopy.distance.geodesic(coords_1, coords_2).miles)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all filenames (stored in a file that is in common to multiple scripts)\n",
    "import all_filenames\n",
    "from all_filenames import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_pin\n",
    "from utils_pin import print_elapsed#, draw_map, draw_map_and_landmarks\n",
    "importMAP = False\n",
    "if importMAP:\n",
    "    import maputils_pin\n",
    "    from maputils_pin import draw_map, draw_map_and_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot saving folder\n",
    "plot_save_dir = '/Users/stefgarasto/Google Drive/Documents/results/PIN/plots/'\n",
    "# file where I'm storing all the information\n",
    "save_oa_file = res_folder + 'PIN/oa_distances_and_occupations_v2.pickle'\n",
    "save_oa_file_jobs = res_folder + 'PIN/oa_jobs_breakdown.pickle'\n",
    "tmp_proper_folder = res_folder + 'PIN/tmp-propeR-data'\n",
    "tmp_proper_results = res_folder + 'PIN/tmp-propeR-res'\n",
    "FIGSAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, load the list of all TTWA\n",
    "ttwa_data = pd.read_csv(ttwa_file)\n",
    "# first column is ttwa codes, second column is ttwa names\n",
    "ttwa_info11 = pd.read_excel(ttwa_info11_file)\n",
    "ttwa_info16 = pd.read_excel(ttwa_info16_file)\n",
    "#print(ttwa_info11.tail(n=3))\n",
    "#print(ttwa_info16.tail(n=3))\n",
    "\n",
    "# get small TTWAs\n",
    "small_ttwas = list(ttwa_info11['ttwa11cd'][ttwa_info11['LSOAs']<40])\n",
    "print('There are {} TTWAs with less than 40 LSOAs.'.format(len(small_ttwas)))\n",
    "\n",
    "# now set the ttwa code as the index\n",
    "ttwa_data = ttwa_data.set_index('ttwa11cd')\n",
    "ttwa_info11 = ttwa_info11.set_index('ttwa11cd')\n",
    "ttwa_info16 = ttwa_info16.set_index('ttwa11cd')\n",
    "\n",
    "# drop rows\n",
    "ttwa_data = ttwa_data.drop(small_ttwas, axis = 0)\n",
    "ttwa_info11 = ttwa_info11.drop(small_ttwas, axis = 0)\n",
    "ttwa_info16 = ttwa_info16.drop([t for t in small_ttwas if t in ttwa_info16.index], axis = 0)\n",
    "ttwa_info16 = ttwa_info16.sort_index()\n",
    "ttwa_info11 = ttwa_info11.sort_index()\n",
    "#ttwa_data['Region/Country'] = ttwa_info16['Region/Country']\n",
    "\n",
    "# Create aliases for the column names (need to be shorter to be plotted correctly)\n",
    "rename_cols16 = {'Employment rate ': 'Employment rate',\n",
    "       '% of economically inactive who want a job':'Job-seeking economically inactive',\n",
    "       'Claimant Count, % aged 16-64, April 2015 to March 2016 ': 'Claimant count',\n",
    "       'All in employment who are 1: managers, directors and senior officials (SOC2010)': \n",
    "                 'Employed in SOC code 1',\n",
    "       ' All in employment who are 2: professional occupations or 3: associate prof & tech occupations (SOC2010)': \n",
    "                 'Employed in SOC code 2',\n",
    "       'All in employment who are 5: skilled trades occupations (SOC2010)': \n",
    "                 'Employed in SOC code 5',\n",
    "       'All in employment who are 6: caring, leisure and other service occupations (SOC2010)': \n",
    "                 'Employed in SOC code 6',\n",
    "       'All in employment who are 8: process, plant and machine operatives (SOC2010)':\n",
    "                 'Employed in SOC code 8',\n",
    "       'All in employment who are 9: elementary occupations (SOC2010)':\n",
    "                 'Employed in SOC code 9'}\n",
    "\n",
    "rename_cols11 = {'Supply-side self-containment (% employed residents who work locally)':\n",
    "                 'Supply-side self-containment',\n",
    "       'Demand-side self-containment (% local jobs taken by local residents)':\n",
    "                 'Demand-side self containment',\n",
    "       'Number of economically active residents (aged 16+)':'Economically active residents'}\n",
    "ttwa_info16.rename(rename_cols16, axis = 1, inplace = True)\n",
    "ttwa_info11.rename(rename_cols11, axis = 1, inplace = True)\n",
    "\n",
    "ttwa_data = ttwa_data.sort_index().join(ttwa_info11, rsuffix = '_2').join(ttwa_info16, \n",
    "                                                                            rsuffix = '_3')\n",
    "\n",
    "ttwa_data = ttwa_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the extracted dictionaries of OA centroids\n",
    "loadOA = True\n",
    "loadLSOA = True\n",
    "oa_path = ons_der_folder + 'oa_centroids_dictionary.pickle'\n",
    "lsoa_path = ons_der_folder + 'lsoa_centroids_dictionary.pickle'\n",
    "exists = os.path.isfile(oa_path)\n",
    "if exists and loadOA:\n",
    "    print('Loading the OA data')\n",
    "    oa_data = pd.read_pickle(oa_path)\n",
    "oa_data.rename(columns = {'long': 'lon'}, inplace = True)\n",
    "\n",
    "exists = os.path.isfile(lsoa_path)\n",
    "if exists and loadLSOA:\n",
    "    print('Loading the LSOA data')\n",
    "    lsoa_data = pd.read_pickle(lsoa_path)\n",
    "lsoa_data.rename(columns = {'long': 'lon'}, inplace = True)\n",
    "\n",
    "# Load the data dictionaries which then should be transformed to dataframes and joined.\n",
    "# They should also be joined with the list of TTWAs for each OA\n",
    "# Then, I can make the relevant plots\n",
    "# What I want is a breakdown of mean travel distances for occupations and for ttwa\n",
    "\n",
    "# first, load the data\n",
    "with open(save_oa_file, 'rb') as f:\n",
    "    _,_,oa_occupations,oa_residents,socGroups,_,_ = pickle.load(f)\n",
    "\n",
    "with open(save_oa_file_jobs, 'rb') as f:\n",
    "    _,oa_number_of_jobs,oa_jobs_breakdown,jobs_socGroups,_,_ = pickle.load(f)\n",
    "    \n",
    "print('Loaded LMIforALL data. Now joining')\n",
    "t0 = time.time()\n",
    "# join all dictionaries with oa_data and delete?\n",
    "# first create the residents column and change the column title\n",
    "oa_data = oa_data.join(pd.DataFrame.from_dict(oa_residents, orient = 'index'))\n",
    "# now add everything else\n",
    "oa_data.rename(columns = {0: 'residents'}, inplace = True)\n",
    "oa_data = oa_data.join(\n",
    "    pd.DataFrame.from_dict(oa_occupations, orient = 'index')).join(\n",
    "    pd.DataFrame.from_dict(oa_number_of_jobs, orient = 'index')).join(\n",
    "    pd.DataFrame.from_dict(oa_jobs_breakdown, orient = 'index'))\n",
    "print('It took {:2f}s to create the full dataframe with {} rows'.format(time.time()- t0, \n",
    "                                                                        len(oa_data)))\n",
    "# finally, rename the number of jobs column\n",
    "oa_data.rename(columns = {0: 'number of jobs'}, inplace = True)\n",
    "print(oa_data.head(n=2))\n",
    "\n",
    "oa_occupations = None\n",
    "oa_residents = None\n",
    "oa_number_of_jobs = None\n",
    "oa_jobs_breakdown = None\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up propeR\n",
    "# set the region to use\n",
    "region2use = 'wm'\n",
    "region_name = 'West Midlands'\n",
    "# open the connection to Open Trip Planner\n",
    "otpcon = propeR.otpConnect(router = 'default_{}'.format(region2use))\n",
    "\n",
    "# [TODO] how to check the connection is open?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the TTWA in the region\n",
    "regional_ttwa = ttwa_data[ttwa_data['Region/Country'] == region_name][['ttwa11cd','ttwa11nm','LSOAs']]\n",
    "print(regional_ttwa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsoa_data.loc['E01008881'])#[lsoa_data['ttwa']==regional_ttwa['ttwa11cd'].iloc[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_oas(ttwa, origin_lsoa, destination_lsoa, oa_data, lsoa_data, n = 3):\n",
    "    '''\n",
    "    This function is needed to sample OAs to be origins and destinations when computing travel\n",
    "    time between two LSOAs.\n",
    "    I will sample n OAs in each LSOA (origin and destination) and return a dataframe with\n",
    "    names and centroids of the selected OAs.\n",
    "    '''\n",
    "    oa_list_origin = lsoa_data.loc[origin_lsoa]['oa_list']\n",
    "    oa_list_destination = lsoa_data.loc[destination_lsoa]['oa_list']\n",
    "    # random selection of indices\n",
    "    ix_origin = random.sample(range(0,len(oa_list_origin)), n)\n",
    "    ix_destination = random.sample(range(0,len(oa_list_destination)), n)\n",
    "    # get the corresponding OAs and add them to a dataframe\n",
    "    tmp_origin = [oa_list_origin[t] for t in ix_origin]\n",
    "    sampled_oa_origin = pd.DataFrame(tmp_origin, columns = ['name'])\n",
    "    tmp_destination = [oa_list_destination[t] for t in ix_destination]\n",
    "    sampled_oa_destination = pd.DataFrame(tmp_destination, columns = ['name'])\n",
    "    sampled_oa_origin = pd.merge(sampled_oa_origin, \n",
    "                            oa_data[['lat', 'lon', 'ttwa' ,'lsoa11']].loc[tmp_origin], \n",
    "                            left_on = 'name', right_index = True)\n",
    "    sampled_oa_destination = pd.merge(sampled_oa_destination, \n",
    "                            oa_data[['lat', 'lon', 'ttwa' ,'lsoa11']].loc[tmp_destination], \n",
    "                            left_on = 'name', right_index = True)\n",
    "    # rename destinations and origins to make sure the name is a unique ID\n",
    "    sampled_oa_destination['name'] = sampled_oa_destination['name'].map(lambda x : 'd' + x)\n",
    "    sampled_oa_origin['name'] = sampled_oa_origin['name'].map(lambda x : 'o' + x)\n",
    "    # reduce number of digits\n",
    "    sampled_oa_origin['lon'] = sampled_oa_origin['lon'].map(lambda x: np.around(x,3))\n",
    "    sampled_oa_origin['lat'] = sampled_oa_origin['lat'].map(lambda x: np.around(x,3))\n",
    "    sampled_oa_destination['lon'] = sampled_oa_destination['lon'].map(lambda x: np.around(x,3))\n",
    "    sampled_oa_destination['lat'] = sampled_oa_destination['lat'].map(lambda x: np.around(x,3))\n",
    "    return sampled_oa_origin, sampled_oa_destination\n",
    "\n",
    "\n",
    "def convert_to_propeR(locations, tmp_proper_folder, tmp_file_name = 'tmp_location_copy.csv', \n",
    "                      remove = False):\n",
    "    '''\n",
    "    This function is to save the dataframe as csv, reload it with propeR and then \n",
    "    delete the file, if necessary\n",
    "    '''\n",
    "    locations.to_csv(os.path.join(tmp_proper_folder, tmp_file_name), index = False)\n",
    "    # now reload them with propeR\n",
    "    locations_df = propeR.importLocationData(os.path.join(tmp_proper_folder, tmp_file_name))\n",
    "    if remove:\n",
    "        os.remove(os.path.join(tmp_proper_folder, tmp_file_name))\n",
    "    return locations_df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, check the distance in miles, using geopy\n",
    "'''\n",
    "flying_crow = {}\n",
    "for t,ttwa in enumerate(regional_ttwa.index):\n",
    "    t0 = time.time()\n",
    "    ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "    all_lsoas = list(lsoa_data[lsoa_data['ttwa']==ttwa_code].index)\n",
    "    flying_crow[ttwa_code] = np.zeros((len(all_lsoas),len(all_lsoas)))\n",
    "    for to, origin_lsoa in enumerate(all_lsoas):\n",
    "        for td, destination_lsoa in enumerate(all_lsoas):\n",
    "            coords_1 = (lsoa_data['lat'].loc[origin_lsoa],lsoa_data['lon'].loc[origin_lsoa])\n",
    "            coords_2 = (lsoa_data['lat'].loc[destination_lsoa],lsoa_data['lon'].loc[destination_lsoa])\n",
    "            flying_crow[ttwa_code][to,td] = geopy.distance.geodesic(coords_1, coords_2).miles\n",
    "    print_elapsed(t0, 'computing distances for {}'.format(regional_ttwa['ttwa11nm'].loc[ttwa]))\n",
    "'''\n",
    "print('unfortunately not useful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ttwa0 = 'E30000202'\n",
    "#plt.hist(np.reshape(flying_crow[ttwa0],(-1,1)))\n",
    "#print(np.sum(flying_crow[ttwa0]<10)/flying_crow[ttwa0].size)\n",
    "#print(flying_crow[ttwa0].size*0.1*10/60/60/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of jobs in each LSOA\n",
    "local_lsoas_number_of_jobs = {}\n",
    "for t,ttwa in enumerate(regional_ttwa.index):\n",
    "    local_lsoas_number_of_jobs[ttwa] = []\n",
    "    t0 = time.time()\n",
    "    ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "    local_lsoa = lsoa_data[lsoa_data['ttwa']==ttwa_code]\n",
    "    for lsoa in local_lsoa.index:\n",
    "        oa_list = local_lsoa['oa_list'].loc[lsoa]\n",
    "        tot_lsoa_jobs = []\n",
    "        for oa in oa_list:\n",
    "            tot_lsoa_jobs.append(oa_data['number of jobs'].loc[oa])\n",
    "        # add the absolute number of jobs\n",
    "        local_lsoas_number_of_jobs[ttwa].append(sum(tot_lsoa_jobs))\n",
    "        #local_lsoa_density_of_jobs.append(np.mean(tot_lsoa_jobs))\n",
    "        #local_lsoa_max_of_jobs.append(max(tot_lsoa_jobs))\n",
    "    # turn the list into a series\n",
    "    local_lsoas_number_of_jobs[ttwa] = pd.DataFrame(local_lsoas_number_of_jobs[ttwa], columns = ['number of jobs'], \n",
    "                                                   index= local_lsoa.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle through all TTWAs in West Midlands and all LSOAs (selecting top 20 destinations given number of jobs)\n",
    "modes = 'WALK, TRANSIT'\n",
    "for t,ttwa in enumerate(regional_ttwa.index[9:10]):\n",
    "    t00 = time.time()\n",
    "    ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "    local_lsoa = lsoa_data[lsoa_data['ttwa']==ttwa_code].join(local_lsoas_number_of_jobs[ttwa]).sort_values(\n",
    "        'number of jobs', ascending = False)\n",
    "    all_lsoas = list(local_lsoa.index)\n",
    "    for to, origin_lsoa in enumerate(all_lsoas):\n",
    "        t0 = time.time()\n",
    "        # only compute travel times for the top 20 destinations\n",
    "        for td, destination_lsoa in enumerate(all_lsoas[:20]):\n",
    "            # select 3 random origins and destinations OA to get travel times for\n",
    "            origin_oas, destination_oas = get_sample_oas(ttwa_code, origin_lsoa, \n",
    "                                                destination_lsoa, oa_data, \n",
    "                                                 lsoa_data, n = 2)\n",
    "            # now convert them to propeR\n",
    "            origins_df = convert_to_propeR(origin_oas, tmp_proper_folder, remove = False)\n",
    "            destinations_df = convert_to_propeR(destination_oas, tmp_proper_folder)\n",
    "            # create a uniquely named folder where to save the output, since I can't control\n",
    "            # the name with which the output is stored\n",
    "            directoryID = os.path.join(tmp_proper_results,\n",
    "                                       'ttwa{}_{}/o{}_d{}_{}'.format(ttwa_code,modes.replace(',','').replace(' ',''), \n",
    "                                                              origin_lsoa,\n",
    "                                                              destination_lsoa,modes.replace(',','').replace(' ','')))\n",
    "            if not os.path.exists(directoryID):\n",
    "                os.makedirs(directoryID)\n",
    "            else:\n",
    "                # overwrite the directory (not sure it's the best method - but it's \n",
    "                # basically the same as overwriting a file)\n",
    "                #shutil.rmtree(directoryID)\n",
    "                #os.makedirs(directoryID)\n",
    "                contents = os.listdir(directoryID)\n",
    "                if len(contents)>0:\n",
    "                    continue\n",
    "                else:\n",
    "                    print(directoryID)\n",
    "                    #stop\n",
    "            # if more than 15 miles (i.e. 25 km then assume the journey takes too long - >25 minutes)\n",
    "            if True: #flying_crow[ttwa_code][to,td]<150:\n",
    "                propeR.pointToPointLoop(directoryID, otpcon = otpcon, \n",
    "                                        originPoints = origins_df,\n",
    "                                        destinationPoints = destinations_df, \n",
    "                                        startDateAndTime = '2019-06-26 16:30:00', \n",
    "                                        modes = modes, journeyReturn = True,\n",
    "                                       preWaitTime = 60)\n",
    "                # for cars it was 2019-06-19 11:30 no return\n",
    "            else:\n",
    "                print('Skipping this pair')\n",
    "                pd.Dataframe.from_dict({'skipping': 1}).to_csv(os.path.join(directoryID, \n",
    "                                                                            'PointToPointLoop_skipped.csv'))\n",
    "            print_elapsed(t0, 'destination {} for origin {}'.format(\n",
    "                destination_lsoa,origin_lsoa))\n",
    "        print(time.time()-t0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time.time() - t00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lsoas.index(destination_lsoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
