{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jobs breakdown per OA\n",
    "This notebook is to call the lmiforall API that returns data on occupation breakdown from the 2011 census.\n",
    "\n",
    "The centroids for each OA, or LSOA, are computed in another script (get_oa_lsoa_centroids) using the ONS postcode directory (February 2019). The same dataset also offers a lookup between OAs and TTWAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "# set the default to darkgrid\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from all_filenames import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 55 TTWAs with less than 40 LSOAs.\n"
     ]
    }
   ],
   "source": [
    "# first, load the list of all TTWA\n",
    "ttwa_data = pd.read_csv(ttwa_file)\n",
    "# first column is ttwa codes, second column is ttwa names\n",
    "ttwa_info11 = pd.read_excel(ttwa_info11_file)\n",
    "ttwa_info16 = pd.read_excel(ttwa_info16_file)\n",
    "#print(ttwa_info11.tail(n=3))\n",
    "#print(ttwa_info16.tail(n=3))\n",
    "\n",
    "# get small TTWAs\n",
    "small_ttwas = list(ttwa_info11['ttwa11cd'][ttwa_info11['LSOAs']<40])\n",
    "print('There are {} TTWAs with less than 40 LSOAs.'.format(len(small_ttwas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadin the OA data\n"
     ]
    }
   ],
   "source": [
    "# load the extracted dictionaries of OA centroids\n",
    "loadOA = True\n",
    "loadLSOA = False\n",
    "oa_path = folder4 + 'oa_centroids_dictionary.pickle'\n",
    "lsoa_path = folder4 + 'lsoa_centroids_dictionary.pickle'\n",
    "exists = os.path.isfile(oa_path)\n",
    "if exists and loadOA:\n",
    "    print('Loadin the OA data')\n",
    "    oa_data = pd.read_pickle(oa_path)\n",
    "\n",
    "exists = os.path.isfile(lsoa_path)\n",
    "if exists and loadLSOA:\n",
    "    print('Loading the LSOA data')\n",
    "    lsoa_data = pd.read_pickle(lsoa_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file where I'm storing all the information\n",
    "save_oa_file = folder3 + 'PIN/oa_jobs_breakdown.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232034\n"
     ]
    }
   ],
   "source": [
    "APICALL = False\n",
    "print(len(oa_data))\n",
    "if APICALL:\n",
    "    \n",
    "    # for each OA, call the LMIforALL API:    \n",
    "    oa_number_of_jobs2 = {}\n",
    "    oa_jobs_breakdown2 = {}\n",
    "    jobs_socGroups = {}\n",
    "    \n",
    "    # it will take multiple hours to gather them all. So, need to create breakpoints\n",
    "    # if file exists load it and start from where it left off\n",
    "    exists = os.path.isfile(save_oa_file)\n",
    "    start_oa = 0\n",
    "\n",
    "    #TODO: actually, there is no need to have the entire pre-stored list while making the new calls, \n",
    "    # only need to join the old and new dictionaries in the end.\n",
    "    if exists:\n",
    "        print('Loading already downloaded OAs')\n",
    "        with open(save_oa_file,'rb') as f:\n",
    "            # I only need to load the indices where we left off last time\n",
    "            _,_,_,jobs_socGroups,_,start_oa = pickle.load(f)\n",
    "        start_oa += 1\n",
    "    end_oa = min(start_oa + 10000, len(oa_data.index))\n",
    "    print(start_oa, end_oa)\n",
    "    if end_oa - start_oa<1:\n",
    "        stop\n",
    "    t0 = time.time()\n",
    "    # loop through all OAs and call the LMIforALL API\n",
    "    N = 499\n",
    "    missing_oas = []\n",
    "    \n",
    "    for ii,oa in enumerate(oa_data.index[start_oa:end_oa]):\n",
    "        if oa[0] in ['L', 'M']:\n",
    "            # exclude these two OAs. They are channel islands and isle of Man, respectively, but they seem to have \n",
    "            # wrong latitude and longitude. I'll check them out better later\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            urlname = 'http://api.lmiforall.org.uk/api/v1/census/jobs_breakdown?area={:6f}%2C{:6f}'.format(\n",
    "            oa_data.loc[oa]['lat'],oa_data.loc[oa]['long'])\n",
    "            out = requests.get(urlname).json()\n",
    "            oa_number_of_jobs2[oa] = out['totalJobs']\n",
    "            out = out['jobsBreakdown']\n",
    "            tmp = {}\n",
    "            for itmp in out:\n",
    "                # use the socGroup as the key (adding value or pecentage), so that then each SOC will become a column\n",
    "                tmp[itmp['socGroup']+'_value'] = itmp['value']\n",
    "                tmp[itmp['socGroup']+'_percentage'] = itmp['percentage']\n",
    "                # at the same time, keep a list of names associated with socgroups\n",
    "                jobs_socGroups[itmp['socGroup']] = itmp['description']\n",
    "            oa_jobs_breakdown2[oa] = tmp\n",
    "            if ii%(N+1) == N:\n",
    "                print('Done with the last {} OAs. It took {:4f} s'.format(N+1, time.time() - t0))\n",
    "                t0 = time.time()\n",
    "        except:\n",
    "            missing_oas.append(oa)\n",
    "            continue\n",
    "\n",
    "    t0 = time.time()\n",
    "    # reload the  previously filled dictionary\n",
    "    if exists:\n",
    "        with open(save_oa_file, 'rb') as f:\n",
    "            _,oa_number_of_jobs,oa_jobs_breakdown,_,_,_ = pickle.load(f)\n",
    "        M = len(oa_number_of_jobs)\n",
    "\n",
    "        # update the dictionaries\n",
    "        oa_number_of_jobs.update(oa_number_of_jobs2)\n",
    "        oa_jobs_breakdown.update(oa_jobs_breakdown2)\n",
    "    else:\n",
    "        oa_number_of_jobs = oa_number_of_jobs2\n",
    "        oa_jobs_breakdown = oa_jobs_breakdown2\n",
    "        \n",
    "    with open(save_oa_file, 'wb') as f:\n",
    "        pickle.dump((oa_data,oa_number_of_jobs,oa_jobs_breakdown,jobs_socGroups,start_oa,start_oa+ii),f)\n",
    "\n",
    "    oa_number_of_jobs = None\n",
    "    oa_jobs_breakdown = None\n",
    "    print('Done. Time spent saving: {:2f} s'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_oas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-041480a907f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmissing_oas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'missing_oas' is not defined"
     ]
    }
   ],
   "source": [
    "missing_oas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_oa_file, 'rb') as f:\n",
    "        _,oa_number_of_jobs,oa_jobs_breakdown,jobs_socGroups,start_oa,end_oa = pickle.load(f)\n",
    "len(oa_number_of_jobs),len(oa_jobs_breakdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_oas = []\n",
    "for oa in oa_data.index:\n",
    "    if oa not in oa_jobs_breakdown:\n",
    "        print(oa)\n",
    "        missing_oas.append(oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_oas = [\n",
    "'S22000047',\n",
    "'S22000049',\n",
    "'S22000051',\n",
    "'S22000054',\n",
    "'S22000055',\n",
    "'S22000056',\n",
    "'S22000057',\n",
    "'S22000059',\n",
    "'S22000060',\n",
    "'S22000061',\n",
    "'S22000063',\n",
    "'S22000065',\n",
    "'S22000067',\n",
    "'S22000068',\n",
    "'S22000069',\n",
    "'S22000070',\n",
    "'S22000071',\n",
    "'S22000074',\n",
    "'S22000075',\n",
    "'S22000078',\n",
    "'S22000085']\n",
    "\n",
    "#'N12000001','N12000002','N12000003','N12000005','N12000006','N12000009','N12000010',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of problematic OAs:  28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefgarasto/miniconda3/envs/transport/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (32,42,47,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'N12000001'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-37cfa4ae940c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmissing_oas_pcd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmissing_oa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing_oas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_oa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmissing_oas_pcd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pcd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transport/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mget_group\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0minds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'N12000001'"
     ]
    }
   ],
   "source": [
    "# There are some OAs that I am missing because the API returns that their LONG/LAT is outside of the UK. \n",
    "# The longitude/latitude pair is computed as the average across postcodes belonging to them\n",
    "# To get the data for these OAs, I call the API using one of the postcodes in them \n",
    "# (the first that the ONS postcode directory returns)\n",
    "\n",
    "GETMISSING = True\n",
    "if GETMISSING:\n",
    "\n",
    "    print('Number of problematic OAs: ', len(missing_oas))\n",
    "    # First, I need to load the ONS postcodes directory and group them by OAs\n",
    "    ons_data = pd.read_csv(ons_pc_file)\n",
    "    groups = ons_data.groupby('oa11')\n",
    "\n",
    "    # get the postcode list for the missing OAs\n",
    "    missing_oas_pcd = []\n",
    "    for missing_oa in missing_oas:\n",
    "        group = groups.get_group(missing_oa)\n",
    "        missing_oas_pcd.append(list(group['pcd']))\n",
    "\n",
    "# now call the LMI API using the first postcode\n",
    "APICALL_missed = False\n",
    "if APICALL_missed and GETMISSING:\n",
    "    # for each OA, call the LMIforALL API:\n",
    "    oa_distances2 = {}\n",
    "    oa_occupations2 = {}\n",
    "    oa_residents2 = {}\n",
    "\n",
    "    # actually, there is no need to have the entire pre-stored list while making the new calls, only need to join \n",
    "    # the old and new dictionaries in the end.\n",
    "    t0 = time.time()\n",
    "    # loop through all OAs and call the LMIforALL API\n",
    "    N = 49\n",
    "    failed_oas = [[], []]\n",
    "    for ii,oa in enumerate(missing_oas):\n",
    "        # still skip Channel Island and Isle of Man\n",
    "        if oa[0] in ['L', 'M']:\n",
    "            continue\n",
    "        # mean distances\n",
    "        if oa == 'S00128975':\n",
    "            ipcd = 1\n",
    "        else:\n",
    "            ipcd = 0\n",
    "        #try:\n",
    "        urlname = 'http://api.lmiforall.org.uk/api/v1/census/jobs_breakdown?area={}'.format(\n",
    "        missing_oas_pcd[ii][ipcd].replace(' ',''))\n",
    "        print(urlname)\n",
    "        out = requests.get(urlname).json()\n",
    "        oa_number_of_jobs2[oa] = out['totalJobs']\n",
    "        out = out['jobsBreakdown']\n",
    "        tmp = {}\n",
    "        for itmp in out:\n",
    "            # use the socGroup as the key (adding value or pecentage), so that then each SOC will become a column\n",
    "            tmp[itmp['socGroup']+'_value'] = itmp['value']\n",
    "            tmp[itmp['socGroup']+'_percentage'] = itmp['percentage']\n",
    "            # at the same time, keep a list of names associated with socgroups\n",
    "            jobs_socGroups[itmp['socGroup']] = itmp['description']\n",
    "        oa_jobs_breakdown2[oa] = tmp\n",
    "        if ii%(N+1) == N:\n",
    "            print('Done with the last {} OAs. It took {:4f} s'.format(N+1, time.time() - t0))\n",
    "            t0 = time.time()\n",
    "#        except:\n",
    "#            print(oa, out, )\n",
    "#            failed_oas[0].append(oa)\n",
    "#            failed_oas[1].append(out)\n",
    "\n",
    "UPDATE = False\n",
    "if UPDATE and APICALLmissed and GETMISSING:\n",
    "    # now reload the  previously filled dictionary\n",
    "    t0 = time.time()\n",
    "    with open(save_oa_file, 'rb') as f:\n",
    "        _,oa_number_of_jobs,oa_jobs_breakdown,jobs_socGroups,_,_ = pickle.load(f)\n",
    "    M = len(oa_number_of_jobs)\n",
    "\n",
    "    # update the dictionaries\n",
    "    oa_number_of_jobs.update(oa_number_of_jobs2)\n",
    "    oa_jobs_breakdown.update(oa_jobs_breakdown2)\n",
    "    #jobs_socGroups2.update(jobs_socGroups)\n",
    "\n",
    "    # save it\n",
    "    with open(save_oa_file, 'wb') as f:\n",
    "        pickle.dump((oa_data,oa_number_of_jobs,oa_jobs_breakdown,jobs_socGroups,start_oa,start_oa+ii),f)\n",
    "\n",
    "    oa_number_of_jobs = None\n",
    "    oa_jobs_breakdown = None\n",
    "    print('Done. Time spent saving: {:2f} s'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_oas_pcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Check memory usage of all variables\n",
    "'''\n",
    "\n",
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') \n",
    "        and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
