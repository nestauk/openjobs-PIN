{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed for propeR\n",
    "TRANSPORT= False\n",
    "if TRANSPORT:\n",
    "    from rpy2.robjects import pandas2ri, numpy2ri\n",
    "    pandas2ri.activate()\n",
    "    numpy2ri.activate()\n",
    "    from rpy2.robjects.packages import importr\n",
    "    propeR = importr('propeR')\n",
    "\n",
    "# general imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "from importlib import reload\n",
    "import random\n",
    "import tempfile\n",
    "import shutil\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# transport\n",
    "if TRANSPORT:\n",
    "    import requests\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all filenames (stored in a file that is in common to multiple scripts)\n",
    "import all_filenames\n",
    "from all_filenames import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_pin\n",
    "from utils_pin import print_elapsed#, draw_map, draw_map_and_landmarks\n",
    "importMAP = True\n",
    "if importMAP:\n",
    "    import maputils_pin\n",
    "    from maputils_pin import draw_map, draw_map_and_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot saving folder\n",
    "plot_save_dir = '/Users/stefgarasto/Google Drive/Documents/results/PIN/plots/propeR/'\n",
    "# file where I'm storing all the information\n",
    "save_oa_file = res_folder + 'PIN/oa_distances_and_occupations_v2.pickle'\n",
    "save_oa_file_jobs = res_folder + 'PIN/oa_jobs_breakdown.pickle'\n",
    "tmp_proper_folder = res_folder + 'PIN/tmp-propeR-data'\n",
    "tmp_proper_results = res_folder + 'PIN/tmp-propeR-res'\n",
    "avg_travel_times_ons = data_folder + 'ONS/datalookingattraveltoworkmethodsandusualhometoworktraveltime.xls'\n",
    "lsoa_to_utla_file = data_folder + 'ONS/Lower_Layer_Super_Output_Area_2011_to_Upper_Tier_Local_Authorities_2017_Lookup_in_England_and_Wales_v2.csv'\n",
    "ons_pc_file = data_folder + 'ONS/OA_to_LSOA_to_MSOA_to_LAD_December2017_Great_Britain.csv'\n",
    "SAVEFIG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ons_pc_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-aad42188297a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlsoa_to_lad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mons_pc_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# aggregate by LSOA and only keep the first LAD (it might not be exact, since the allocation unit is the postcode)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m lsoa_to_lad = ons_pc_data[['LAD17CD','LAD17NM','LSOA11CD','LSOA11NM']].groupby(by= 'LSOA11CD', \n\u001b[0m\u001b[1;32m      5\u001b[0m                                                                     as_index = False).agg(lambda x: x.iloc[0])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ons_pc_data' is not defined"
     ]
    }
   ],
   "source": [
    "# load the lookup table\n",
    "lsoa_to_lad = pd.read_csv(ons_pc_file)\n",
    "# aggregate by LSOA and only keep the first LAD (it might not be exact, since the allocation unit is the postcode)\n",
    "lsoa_to_lad = ons_pc_data[['LAD17CD','LAD17NM','LSOA11CD','LSOA11NM']].groupby(by= 'LSOA11CD', \n",
    "                                                                    as_index = False).agg(lambda x: x.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsoa_to_utla = pd.read_csv(lsoa_to_utla_file)\n",
    "#lsoa_to_utla[lsoa_to_utla['UTLA18NM'] == 'North Somerset'].iloc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load official statistics about average commuting times and modes of travel per local authority\n",
    "def convert_string(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "df_ons = pd.read_excel(avg_travel_times_ons, sheet_name='2016 clean')\n",
    "print(df_ons.head(n=3))\n",
    "all_ons_times = df['Mean']\n",
    "median_travel_times = all_ons_times.median()\n",
    "fig, ax = plt.subplots(1,2, figsize = (10,4))\n",
    "ax[0].hist(all_ons_times) #.plot(kind= 'hist', ax = ax[0])\n",
    "#plt.set_gca(ax[0])\n",
    "ax[0].set_xlabel('Average commuting time', fontsize = 14)\n",
    "ax[0].set_ylabel('Frequency', fontsize = 14)\n",
    "#print(list(ax[0].get_yticklabels()))\n",
    "#ax[0].set_yticklabels(ax[0].get_yticklabels(), {'fontsize' : 12})\n",
    "#ax[0].set_xticklabels(ax[0].get_xticklabels(), fontsize = 12)\n",
    "median_modes = {}\n",
    "all_modes = [' Car,van,minibus,works van', ' Motorbike,moped,scooter',\n",
    "       ' Bicycle', ' Bus,coach,private bus', 'Taxi', ' Railway train',\n",
    "       ' Underground train,light railway,tram', ' Walk', ' Other method']\n",
    "for col in all_modes:\n",
    "    median_modes[col] = np.nanmean(np.array(df[col].map(lambda x: convert_string(x)).values))\n",
    "ax[1].barh(range(1,10),[t for t in median_modes.values()], tick_label = [t for t in median_modes.keys()])\n",
    "#plt.gca().tick_params(rotation = 90)\n",
    "plt.xlabel('UK average', fontsize = 14)\n",
    "tmp = plt.ylabel('Mode of transport', fontsize = 14)\n",
    "plt.gca().set_yticklabels(plt.gca().get_yticklabels(), fontsize = 12)\n",
    "plt.gca().set_xticklabels(plt.gca().get_xticklabels(), fontsize = 12)\n",
    "plt.tight_layout()\n",
    "if SAVEFIG:\n",
    "    plt.savefig(os.path.join(plot_save_dir,'travel_times_mode_LA_ons_stats.png'), \n",
    "                    bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 55 TTWAs with less than 40 LSOAs.\n"
     ]
    }
   ],
   "source": [
    "# first, load the list of all TTWA\n",
    "ttwa_data = pd.read_csv(ttwa_file)\n",
    "# first column is ttwa codes, second column is ttwa names\n",
    "ttwa_info11 = pd.read_excel(ttwa_info11_file)\n",
    "ttwa_info16 = pd.read_excel(ttwa_info16_file)\n",
    "#print(ttwa_info11.tail(n=3))\n",
    "#print(ttwa_info16.tail(n=3))\n",
    "\n",
    "# get small TTWAs\n",
    "small_ttwas = list(ttwa_info11['ttwa11cd'][ttwa_info11['LSOAs']<40])\n",
    "print('There are {} TTWAs with less than 40 LSOAs.'.format(len(small_ttwas)))\n",
    "\n",
    "# now set the ttwa code as the index\n",
    "ttwa_data = ttwa_data.set_index('ttwa11cd')\n",
    "ttwa_info11 = ttwa_info11.set_index('ttwa11cd')\n",
    "ttwa_info16 = ttwa_info16.set_index('ttwa11cd')\n",
    "\n",
    "# drop rows\n",
    "ttwa_data = ttwa_data.drop(small_ttwas, axis = 0)\n",
    "ttwa_info11 = ttwa_info11.drop(small_ttwas, axis = 0)\n",
    "ttwa_info16 = ttwa_info16.drop([t for t in small_ttwas if t in ttwa_info16.index], axis = 0)\n",
    "ttwa_info16 = ttwa_info16.sort_index()\n",
    "ttwa_info11 = ttwa_info11.sort_index()\n",
    "#ttwa_data['Region/Country'] = ttwa_info16['Region/Country']\n",
    "\n",
    "# Create aliases for the column names (need to be shorter to be plotted correctly)\n",
    "rename_cols16 = {'Employment rate ': 'Employment rate',\n",
    "       '% of economically inactive who want a job':'Job-seeking economically inactive',\n",
    "       'Claimant Count, % aged 16-64, April 2015 to March 2016 ': 'Claimant count',\n",
    "       'All in employment who are 1: managers, directors and senior officials (SOC2010)': \n",
    "                 'Employed in SOC code 1',\n",
    "       ' All in employment who are 2: professional occupations or 3: associate prof & tech occupations (SOC2010)': \n",
    "                 'Employed in SOC code 2',\n",
    "       'All in employment who are 5: skilled trades occupations (SOC2010)': \n",
    "                 'Employed in SOC code 5',\n",
    "       'All in employment who are 6: caring, leisure and other service occupations (SOC2010)': \n",
    "                 'Employed in SOC code 6',\n",
    "       'All in employment who are 8: process, plant and machine operatives (SOC2010)':\n",
    "                 'Employed in SOC code 8',\n",
    "       'All in employment who are 9: elementary occupations (SOC2010)':\n",
    "                 'Employed in SOC code 9'}\n",
    "\n",
    "rename_cols11 = {'Supply-side self-containment (% employed residents who work locally)':\n",
    "                 'Supply-side self-containment',\n",
    "       'Demand-side self-containment (% local jobs taken by local residents)':\n",
    "                 'Demand-side self containment',\n",
    "       'Number of economically active residents (aged 16+)':'Economically active residents'}\n",
    "ttwa_info16.rename(rename_cols16, axis = 1, inplace = True)\n",
    "ttwa_info11.rename(rename_cols11, axis = 1, inplace = True)\n",
    "\n",
    "ttwa_data = ttwa_data.sort_index().join(ttwa_info11, rsuffix = '_2').join(ttwa_info16, \n",
    "                                                                            rsuffix = '_3')\n",
    "\n",
    "ttwa_data = ttwa_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the OA data\n",
      "Loading the LSOA data\n",
      "Loaded LMIforALL data. Now joining\n",
      "It took 160.575711s to create the full dataframe with 232034 rows\n",
      "                 lat       lon       ttwa     lsoa11  residents  \\\n",
      "oa11                                                              \n",
      "E00000001  51.520345 -0.094809  E30000234  E01000001      102.0   \n",
      "E00000003  51.519846 -0.096589  E30000234  E01000001      147.0   \n",
      "\n",
      "           Mean distance to work (overall)      code  \\\n",
      "oa11                                                   \n",
      "E00000001                         4.902913  dist2549   \n",
      "E00000003                         5.510204  dist2549   \n",
      "\n",
      "           Mean distance to work (males)  Mean distance to work (females)  \\\n",
      "oa11                                                                        \n",
      "E00000001                            4.5                         5.465116   \n",
      "E00000003                            5.0                         6.171875   \n",
      "\n",
      "           Mean distance to work (ages 16-24)  ...  245_value  245_percentage  \\\n",
      "oa11                                           ...                              \n",
      "E00000001                            3.000000  ...        0.0             0.0   \n",
      "E00000003                            4.285714  ...        0.0             0.0   \n",
      "\n",
      "           124_value  124_percentage  121_value  121_percentage  118_value  \\\n",
      "oa11                                                                         \n",
      "E00000001        0.0             0.0        0.0             0.0        0.0   \n",
      "E00000003        0.0             0.0        0.0             0.0        0.0   \n",
      "\n",
      "           118_percentage  117_value  117_percentage  \n",
      "oa11                                                  \n",
      "E00000001             0.0        0.0             0.0  \n",
      "E00000003             0.0        0.0             0.0  \n",
      "\n",
      "[2 rows x 239 columns]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# load the extracted dictionaries of OA centroids\n",
    "loadOA = True\n",
    "loadLSOA = True\n",
    "oa_path = ons_der_folder + 'oa_centroids_dictionary.pickle'\n",
    "lsoa_path = ons_der_folder + 'lsoa_centroids_dictionary.pickle'\n",
    "exists = os.path.isfile(oa_path)\n",
    "if exists and loadOA:\n",
    "    print('Loading the OA data')\n",
    "    oa_data = pd.read_pickle(oa_path)\n",
    "oa_data.rename(columns = {'long': 'lon'}, inplace = True)\n",
    "\n",
    "exists = os.path.isfile(lsoa_path)\n",
    "if exists and loadLSOA:\n",
    "    print('Loading the LSOA data')\n",
    "    lsoa_data = pd.read_pickle(lsoa_path)\n",
    "lsoa_data.rename(columns = {'long': 'lon'}, inplace = True)\n",
    "\n",
    "# Load the data dictionaries which then should be transformed to dataframes and joined.\n",
    "# They should also be joined with the list of TTWAs for each OA\n",
    "# Then, I can make the relevant plots\n",
    "# What I want is a breakdown of mean travel distances for occupations and for ttwa\n",
    "\n",
    "# first, load the data\n",
    "with open(save_oa_file, 'rb') as f:\n",
    "    _,oa_distances,oa_occupations,oa_residents,socGroups,_,_ = pickle.load(f)\n",
    "\n",
    "with open(save_oa_file_jobs, 'rb') as f:\n",
    "    _,oa_number_of_jobs,oa_jobs_breakdown,jobs_socGroups,_,_ = pickle.load(f)\n",
    "    \n",
    "print('Loaded LMIforALL data. Now joining')\n",
    "t0 = time.time()\n",
    "# join all dictionaries with oa_data and delete?\n",
    "# first create the residents column and change the column title\n",
    "oa_data = oa_data.join(pd.DataFrame.from_dict(oa_residents, orient = 'index'))\n",
    "# now add everything else\n",
    "oa_data.rename(columns = {0: 'residents'}, inplace = True)\n",
    "oa_data = oa_data.join(\n",
    "    pd.DataFrame.from_dict(oa_distances, orient = 'index')).join(\n",
    "    pd.DataFrame.from_dict(oa_occupations, orient = 'index')).join(\n",
    "    pd.DataFrame.from_dict(oa_number_of_jobs, orient = 'index')).join(\n",
    "    pd.DataFrame.from_dict(oa_jobs_breakdown, orient = 'index'))\n",
    "print('It took {:2f}s to create the full dataframe with {} rows'.format(time.time()- t0, \n",
    "                                                                        len(oa_data)))\n",
    "# finally, rename the number of jobs column\n",
    "oa_data.rename(columns = {0: 'number of jobs'}, inplace = True)\n",
    "print(oa_data.head(n=2))\n",
    "\n",
    "oa_occupations = None\n",
    "oa_residents = None\n",
    "oa_number_of_jobs = None\n",
    "oa_jobs_breakdown = None\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select TTWAs of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the region to use\n",
    "region2use = 'wm'\n",
    "region_name = 'West Midlands'\n",
    "if TRANSPORT:\n",
    "    # set up propeR\n",
    "    # open the connection to Open Trip Planner\n",
    "    otpcon = propeR.otpConnect(router = 'default_{}'.format(region2use))\n",
    "\n",
    "    # [TODO] how to check the connection is open?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ttwa11cd                     ttwa11nm  LSOAs\n",
      "23   E30000169                   Birmingham   1039\n",
      "45   E30000195                     Coventry    367\n",
      "51   E30000202                       Dudley    338\n",
      "63   E30000216                     Hereford    100\n",
      "72   E30000228               Leamington Spa    151\n",
      "101  E30000262                   Shrewsbury     94\n",
      "107  E30000271                     Stafford     95\n",
      "109  E30000273               Stoke-on-Trent    336\n",
      "114  E30000278                      Telford    138\n",
      "123  E30000288    Wolverhampton and Walsall    466\n",
      "124  E30000289  Worcester and Kidderminster    198\n"
     ]
    }
   ],
   "source": [
    "# get all the TTWA in the region\n",
    "regional_ttwa = ttwa_data[ttwa_data['Region/Country'] == region_name][['ttwa11cd','ttwa11nm','LSOAs']]\n",
    "# collect the TTWA names\n",
    "ttwa_formal_names = {}\n",
    "for t in regional_ttwa.index:\n",
    "    ttwa_formal_names[t] = regional_ttwa['ttwa11nm'].loc[t]\n",
    "print(regional_ttwa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the LSOA names files\n",
    "lsoa_names_file = data_folder + 'ONS/Lower_Layer_Super_Output_Areas_December_2011_Names_and_Codes_in_England_and_Wales.csv'\n",
    "lsoa_names = pd.read_csv(lsoa_names_file)\n",
    "lsoa_names = lsoa_names.set_index('LSOA11CD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coversion from TTWA to LAD and viceversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Stratford-on-Avon', 'Sandwell', 'Birmingham', 'Wychavon', 'Redditch',\n",
      "       'North Warwickshire', 'Bromsgrove', 'Lichfield', 'Tamworth', 'Walsall',\n",
      "       'Solihull', 'Daventry', 'Nuneaton and Bedworth', 'Rugby', 'Warwick',\n",
      "       'Coventry', 'Hinckley and Bosworth', 'Dudley', 'Wyre Forest',\n",
      "       'South Staffordshire', 'Herefordshire, County of', 'Cotswold',\n",
      "       'Shropshire', 'Cannock Chase', 'Stafford', 'Newcastle-under-Lyme',\n",
      "       'Cheshire East', 'East Staffordshire', 'Staffordshire Moorlands',\n",
      "       'Stoke-on-Trent', 'Telford and Wrekin', 'Wolverhampton',\n",
      "       'Malvern Hills', 'Worcester'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# try to match upper tier local authority to TTWA through LSOAs\n",
    "groups = lsoa_data.groupby('ttwa')\n",
    "ttwa_to_utlas = {}\n",
    "all_utlas = []\n",
    "for jj,ttwa in enumerate(regional_ttwa.index):\n",
    "    ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "    ttwa_name = regional_ttwa['ttwa11nm'].loc[ttwa]\n",
    "    # get the group corresponding to this TTWA\n",
    "    group = groups.get_group(ttwa_code)\n",
    "    utlas = []\n",
    "    ttwa_to_utlas[ttwa_name] = []\n",
    "    for lsoa in group.index:\n",
    "        ttwa_to_utlas[ttwa_name].append(lsoa_to_lad['LAD17NM'][lsoa_to_lad['LSOA11CD']==lsoa].values[0])\n",
    "    all_utlas = all_utlas + list(set(ttwa_to_utlas[ttwa_name]))\n",
    "ttwa_utlas = {}\n",
    "for jj,ttwa in enumerate(regional_ttwa.index):\n",
    "    ttwa_code = regional_ttwa['ttwa11nm'].loc[ttwa]\n",
    "    ttwa_name = regional_ttwa['ttwa11nm'].loc[ttwa]\n",
    "    ttwa_utlas[ttwa_name] = {}\n",
    "    for utla in all_utlas:\n",
    "        ttwa_utlas[ttwa_name][utla] = np.sum([t == utla for t in ttwa_to_utlas[ttwa_name]])\n",
    "print(pd.DataFrame.from_dict(ttwa_utlas,orient = 'index').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Leamington Spa': ['Stratford-on-Avon', 'Warwick'], 'Dudley': ['Sandwell', 'Dudley'], 'Birmingham': ['Birmingham', 'Redditch', 'North Warwickshire', 'Bromsgrove', 'Tamworth', 'Solihull'], 'Worcester and Kidderminster': ['Wychavon', 'Wyre Forest', 'Malvern Hills', 'Worcester'], 'Wolverhampton and Walsall': ['Lichfield', 'Walsall', 'South Staffordshire', 'Cannock Chase', 'Wolverhampton'], 'Coventry': ['Nuneaton and Bedworth', 'Rugby', 'Coventry'], 'Hereford': ['Herefordshire, County of'], 'Shrewsbury': ['Shropshire'], 'Stafford': ['Stafford'], 'Stoke-on-Trent': ['Newcastle-under-Lyme', 'Cheshire East', 'East Staffordshire', 'Staffordshire Moorlands', 'Stoke-on-Trent'], 'Telford': ['Telford and Wrekin']} {'Stratford-on-Avon': 'Leamington Spa', 'Sandwell': 'Dudley', 'Birmingham': 'Birmingham', 'Wychavon': 'Worcester and Kidderminster', 'Redditch': 'Birmingham', 'North Warwickshire': 'Birmingham', 'Bromsgrove': 'Birmingham', 'Lichfield': 'Wolverhampton and Walsall', 'Tamworth': 'Birmingham', 'Walsall': 'Wolverhampton and Walsall', 'Solihull': 'Birmingham', 'Nuneaton and Bedworth': 'Coventry', 'Rugby': 'Coventry', 'Warwick': 'Leamington Spa', 'Coventry': 'Coventry', 'Dudley': 'Dudley', 'Wyre Forest': 'Worcester and Kidderminster', 'South Staffordshire': 'Wolverhampton and Walsall', 'Herefordshire, County of': 'Hereford', 'Shropshire': 'Shrewsbury', 'Cannock Chase': 'Wolverhampton and Walsall', 'Stafford': 'Stafford', 'Newcastle-under-Lyme': 'Stoke-on-Trent', 'Cheshire East': 'Stoke-on-Trent', 'East Staffordshire': 'Stoke-on-Trent', 'Staffordshire Moorlands': 'Stoke-on-Trent', 'Stoke-on-Trent': 'Stoke-on-Trent', 'Telford and Wrekin': 'Telford', 'Wolverhampton': 'Wolverhampton and Walsall', 'Malvern Hills': 'Worcester and Kidderminster', 'Worcester': 'Worcester and Kidderminster'}\n"
     ]
    }
   ],
   "source": [
    "# create the dataframe\n",
    "tmp = pd.DataFrame.from_dict(ttwa_utlas,orient = 'index')#, columns = la_columns)\n",
    "# only keep those LAD that overlap with at least 10 LSOAs in total with all the TTWAs\n",
    "tmp = tmp[tmp.columns[tmp.sum()>10]]\n",
    "la_columns = tmp.columns #['Walsall',  'Worcestershire',  \n",
    "#            'Sandwell',  'Birmingham', 'Solihull','Staffordshire',\n",
    "#           'Warwickshire','Coventry', 'Dudley', 'Herefordshire, County of', \n",
    "#          'Shropshire',  'Stoke-on-Trent', 'Telford and Wrekin',  'Wolverhampton']\n",
    "ttwa_to_la = {}\n",
    "la_to_ttwa = {}\n",
    "for la in la_columns:\n",
    "    # just one exception\n",
    "    if la == 'Staffordshire':\n",
    "        print('exception')\n",
    "        la_to_ttwa[la] = 'Stafford'\n",
    "    else:\n",
    "        # match the local authority to the TTWA with the most overlap\n",
    "        # (i.e. that contains most of the LSOAs in that LA)\n",
    "        la_to_ttwa[la] = tmp[tmp[la] == tmp[la].max()].index[0]\n",
    "    # now add the LA to the list of LAs in that TTWA\n",
    "    if la_to_ttwa[la] in ttwa_to_la.keys():\n",
    "        ttwa_to_la[la_to_ttwa[la]] += [la]\n",
    "    else:\n",
    "        ttwa_to_la[la_to_ttwa[la]] = [la]\n",
    "    \n",
    "print(ttwa_to_la, la_to_ttwa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0ac0cc8a402e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# cycle through the local authorities in the ONS dataset. They're in the form code + name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdivider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlacd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Look for the ones with the right name,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# ttwa_to_la[ttwa] gives the list of local authorities in this TTWA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ons' is not defined"
     ]
    }
   ],
   "source": [
    "# now assign average commuting times from ONS to each TTWA\n",
    "#print(list(df_ons['Unnamed: 0']))\n",
    "avg_ons_times = {}\n",
    "for ttwa in ttwa_to_la.keys():\n",
    "    tmp = 0\n",
    "    ttwa_code = list(regional_ttwa[regional_ttwa['ttwa11nm']==ttwa]['ttwa11cd'].values)[0]\n",
    "    # cycle through the local authorities in the ONS dataset. They're in the form code + name.\n",
    "    divider = 0\n",
    "    for k,lacd in enumerate(df_ons['Unnamed: 0']):\n",
    "        # Look for the ones with the right name, \n",
    "        # ttwa_to_la[ttwa] gives the list of local authorities in this TTWA\n",
    "        tmp2 = [t in lacd for t in ttwa_to_la[ttwa]]\n",
    "        if any(tmp2):\n",
    "            #print(lacd,k,ttwa,ttwa_to_la[ttwa])\n",
    "            # add the average commuting time in that LA\n",
    "            tmp += df['Mean'].iloc[k]\n",
    "            divider += 1 \n",
    "            # now divide by the # of LAs\n",
    "    tmp = tmp / divider #len(ttwa_to_la[ttwa])\n",
    "    avg_ons_times[ttwa_code] = tmp\n",
    "print(avg_ons_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, get the distance in miles, using geopy\n",
    "flying_crow = {}\n",
    "for t,ttwa in enumerate(regional_ttwa.index):\n",
    "    t0 = time.time()\n",
    "    ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "    all_lsoas = list(lsoa_data[lsoa_data['ttwa']==ttwa_code].index)\n",
    "    flying_crow[ttwa_code] = np.zeros((len(all_lsoas),len(all_lsoas)))\n",
    "    for to, origin_lsoa in enumerate(all_lsoas):\n",
    "        for td, destination_lsoa in enumerate(all_lsoas):\n",
    "            coords_1 = (lsoa_data['lat'].loc[origin_lsoa],lsoa_data['lon'].loc[origin_lsoa])\n",
    "            coords_2 = (lsoa_data['lat'].loc[destination_lsoa],lsoa_data['lon'].loc[destination_lsoa])\n",
    "            flying_crow[ttwa_code][to,td] = geodesic(coords_1, coords_2).miles\n",
    "    print_elapsed(t0, 'computing distances for {}'.format(regional_ttwa['ttwa11nm'].loc[ttwa]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of jobs in each LSOA\n",
    "local_lsoas_number_of_jobs = {}\n",
    "for t,ttwa in enumerate(regional_ttwa.index):\n",
    "    local_lsoas_number_of_jobs[ttwa] = []\n",
    "    t0 = time.time()\n",
    "    ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "    local_lsoa = lsoa_data[lsoa_data['ttwa']==ttwa_code]\n",
    "    for lsoa in local_lsoa.index:\n",
    "        oa_list = local_lsoa['oa_list'].loc[lsoa]\n",
    "        tot_lsoa_jobs = []\n",
    "        for oa in oa_list:\n",
    "            tot_lsoa_jobs.append(oa_data['number of jobs'].loc[oa])\n",
    "        # add the absolute number of jobs\n",
    "        local_lsoas_number_of_jobs[ttwa].append(sum(tot_lsoa_jobs))\n",
    "        #local_lsoa_density_of_jobs.append(np.mean(tot_lsoa_jobs))\n",
    "        #local_lsoa_max_of_jobs.append(max(tot_lsoa_jobs))\n",
    "    # turn the list into a series\n",
    "    local_lsoas_number_of_jobs[ttwa] = pd.DataFrame(local_lsoas_number_of_jobs[ttwa], columns = ['number of jobs'], \n",
    "                                                   index= local_lsoa.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the data already collected for this region and organise it into a better format\n",
    "LOAD_JOINED = False\n",
    "modes = 'WALK, TRANSIT'\n",
    "journey_time = '0730'\n",
    "fmodes = modes.replace(' ','').replace(',','')\n",
    "lmodes = modes.lower().replace(' ','').replace(',','-')\n",
    "if modes == 'WALK, TRANSIT':\n",
    "    lmodes = lmodes + '_' + journey_time\n",
    "    \n",
    "if LOAD_JOINED:\n",
    "    # load the already joined data\n",
    "    with open(os.path.join(tmp_proper_results,'all_data_for_{}_and_{}.pickle'.format(\n",
    "        region_name.replace(' ',''),lmodes)), 'rb') as f:\n",
    "        all_durations, all_distances, origin_lsoas_coord, dest_lsoas_coord, origin_lsoas_names, dest_lsoas_names, origin_lsoas, dest_lsoas = pickle.load(f)\n",
    "else:\n",
    "    # load each separate file, then save it\n",
    "    # initialise dictionaries of matrices\n",
    "    all_durations = {} # this contains all the duration matrices\n",
    "    all_distances = {}\n",
    "    origin_lsoas_coord = {}\n",
    "    dest_lsoas_coord = {}\n",
    "    origin_lsoas_names = {}\n",
    "    dest_lsoas_names = {}\n",
    "    origin_lsoas = {}\n",
    "    dest_lsoas = {}\n",
    "    for t,ttwa in enumerate(regional_ttwa.index):\n",
    "        t0 = time.time()\n",
    "        ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "        local_lsoa = lsoa_data[lsoa_data['ttwa']==ttwa_code].join(local_lsoas_number_of_jobs[ttwa]).sort_values(\n",
    "            'number of jobs', ascending = False)\n",
    "        all_lsoas = list(local_lsoa.index)\n",
    "        all_durations[ttwa_code] = np.zeros((20,len(all_lsoas)))\n",
    "        all_distances[ttwa_code] = np.zeros((20,len(all_lsoas)))\n",
    "        for to, origin_lsoa in enumerate(all_lsoas):\n",
    "            # only compute travel times for the top 20 destinations\n",
    "            for td, destination_lsoa in enumerate(all_lsoas[:20]):\n",
    "\n",
    "                # set the name for the folder where the output was saved, since I can't control\n",
    "                # the name with which the output is stored\n",
    "                directoryID = os.path.join(tmp_proper_results,\n",
    "                                           'ttwa{}_{}/o{}_d{}_{}'.format(ttwa_code,modes.replace(',','').replace(' ',''), \n",
    "                                                                  origin_lsoa,\n",
    "                                                                  destination_lsoa,modes.replace(',','').replace(' ','')))\n",
    "                if  os.path.exists(directoryID):\n",
    "                    content = os.listdir(directoryID)\n",
    "                    # there should be only one .csv file: select it\n",
    "                    content = [t for t in content if t.endswith('.csv')]\n",
    "                    if len(content) != 1:\n",
    "                        print('Houston, we have a problem. ')\n",
    "                        print(directoryID)\n",
    "                        stop\n",
    "                    # load the csv data\n",
    "                    df = pd.read_csv(os.path.join(directoryID, content[0]))\n",
    "                    # combine the data from all pairs of origin and destination OAs\n",
    "                    distances = np.array(df['distance_km'].values)\n",
    "                    durations = np.array(df['duration_mins'].values)\n",
    "                    # filter out any NaNs\n",
    "                    distances = distances[~np.isnan(durations)]\n",
    "                    durations = durations[~np.isnan(durations)]\n",
    "                    # take the average\n",
    "                    if len(distances):\n",
    "                        all_distances[ttwa_code][td,to] = np.mean(distances)\n",
    "                        all_durations[ttwa_code][td,to] = np.mean(durations)\n",
    "                    else:\n",
    "                        all_distances[ttwa_code][td,to] = np.nan\n",
    "                        all_durations[ttwa_code][td,to] = np.nan\n",
    "                else:\n",
    "                    # somehow this data wasn't collected!\n",
    "                    print('Some data was not collected. Problems arose for: ')\n",
    "                    print(directoryID, origin_lsoa, destination_lsoa, ttwa_code)\n",
    "                    #stop\n",
    "                    # set these data as NaN?\n",
    "                    continue\n",
    "                # also, collect the coordinates\n",
    "                origin_lsoas_coord[ttwa_code] = [(local_lsoa['lon'].loc[t],local_lsoa['lat'].loc[t]) \n",
    "                                                 for t in all_lsoas]\n",
    "                dest_lsoas_coord[ttwa_code] = [(local_lsoa['lon'].loc[t],local_lsoa['lat'].loc[t]) \n",
    "                                               for t in all_lsoas[:20]]\n",
    "                # turn codes into names\n",
    "                origin_lsoas_names[ttwa_code] = lsoa_names['LSOA11NM'].loc[all_lsoas].values\n",
    "                dest_lsoas_names[ttwa_code] = lsoa_names['LSOA11NM'].loc[all_lsoas[:20]].values\n",
    "                origin_lsoas[ttwa_code] = all_lsoas\n",
    "                dest_lsoas[ttwa_code] = all_lsoas[:20]\n",
    "            #print_elapsed(t0, 'getting the data for LSOA {} of TTWA {}'.format(origin_lsoa,ttwa_code))\n",
    "        print_elapsed(t0, 'getting the data for TTWA {}'.format(ttwa_code))\n",
    "\n",
    "    # save the data as one pickle files, so it doesn't need to be downloaded again\n",
    "    with open(os.path.join(tmp_proper_results,'all_data_for_{}_and_{}.pickle'.format(\n",
    "        region_name.replace(' ',''),lmodes)), 'wb') as f:\n",
    "        pickle.dump((all_durations, all_distances, origin_lsoas_coord, dest_lsoas_coord, \n",
    "                     origin_lsoas_names, dest_lsoas_names, origin_lsoas, dest_lsoas),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions for plotting (individual TTWAs)\n",
    "'''\n",
    "def plot_durations_heatmap(duration_matrix, Th, origin_lsoas_labels, dest_lsoas_labels, \n",
    "                           ttwa_name, mode, plot_save_dir ='', SAVEFIG = False, which_api = 'OTP'):\n",
    "    '''\n",
    "    # heatmap of thresholded durations for one TTWA\n",
    "    '''\n",
    "    with sns.axes_style('white'):\n",
    "        with sns.plotting_context('paper', font_scale = 1.3):\n",
    "            fig, ax = plt.subplots(figsize=(23,6))\n",
    "            g = sns.heatmap(duration_matrix, xticklabels=origin_lsoas_labels, \n",
    "                            yticklabels=dest_lsoas_labels, \n",
    "                        mask= (duration_matrix > Th) | (duration_matrix==0), \n",
    "                        square = False, vmax = Th,\n",
    "                           cbar_kws={'label':'Average commuting time for {} journeys'.format(mode)})\n",
    "            plt.xlabel('Origins, where people live ({})'.format(ttwa_name), fontsize = 20)\n",
    "            plt.ylabel('Destinations. where jobs are ({})'.format(ttwa_name), fontsize = 20)\n",
    "            plt.gca().set_yticklabels(plt.gca().get_yticklabels(), fontsize = 14)\n",
    "            plt.gca().set_xticklabels(plt.gca().get_xticklabels(), fontsize = 12)\n",
    "            cax = plt.gcf().axes[-1]\n",
    "            cax.tick_params(labelsize=14)\n",
    "            plt.tight_layout()\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(os.path.join(plot_save_dir,'thresholded_commute_time_matrix_{}_{}_{}.png'.format(\n",
    "            ttwa_name,mode,which_api)), bbox_inches = 'tight')\n",
    "    plt.draw()\n",
    "\n",
    "# histograms     \n",
    "def plot_population_hist(population_vector, ttwa_formal_name, plot_save_dir ='', SAVEFIG = False):\n",
    "    with sns.plotting_context('talk'):\n",
    "        fig = plt.figure(figsize = (6,4))\n",
    "        sns.distplot(population_vector, axlabel = 'population in a given LSOA in {}'.format(ttwa_formal_name))\n",
    "    tmp = plt.ylabel('LSOA density')\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(os.path.join(plot_save_dir,'population_by_LSOA_{}.png'.format(ttwa_formal_name)), \n",
    "                    bbox_inches = 'tight')\n",
    "    plt.draw()\n",
    "\n",
    "def plot_jobs_hist(vector, ttwa_formal_name, plot_save_dir ='', \n",
    "                   SAVEFIG = False, t_rotation = 45, nbins = 10):\n",
    "    # this is to plot a count histogram with no overlap estimated pdf (used for jobs vector and Ajt)\n",
    "    with sns.plotting_context('talk'):\n",
    "        fig = plt.figure(figsize = (6,4))\n",
    "        tmp = sns.distplot(vector, axlabel = 'number of jobs in a given LSOA in {}'.format(ttwa_formal_name), \n",
    "                           bins = nbins, norm_hist = False, kde = False)\n",
    "    plt.xticks(rotation = t_rotation)\n",
    "    tmp = plt.ylabel('LSOA count')\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(os.path.join(plot_save_dir, 'jobs_by_destination_LSOA_{}.png'.format(ttwa_formal_name)), \n",
    "                    bbox_inches = 'tight')\n",
    "    plt.draw()\n",
    "\n",
    "def plot_dest_accessibility_hist(vector, ttwa_formal_name, mode, plot_save_dir ='', \n",
    "                   SAVEFIG = False, t_rotation = 45, nbins = 5, which_api = 'OTP'):\n",
    "    # this is to plot a count histogram with no overlap estimated pdf (used for jobs vector and Ajt)\n",
    "    with sns.plotting_context('talk'):\n",
    "        fig = plt.figure(figsize = (6,4))\n",
    "        tmp = sns.distplot(vector, bins = nbins, norm_hist = False, kde = False,\n",
    "                           axlabel = 'Percentage of people less than 45 minutes away ({})'.format(ttwa_formal_name))\n",
    "    plt.xticks(rotation = t_rotation)\n",
    "    tmp = plt.ylabel('LSOA count')\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(os.path.join(plot_save_dir, 'ease_of_commute_distribution_{}_{}_{}.png'.format(\n",
    "            ttwa_formal_name,mode,which_api)), bbox_inches = 'tight')\n",
    "    plt.draw()\n",
    "\n",
    "# plot cumulative accessibility as a map\n",
    "def plot_accessibility_map(Bit_df, lsoa_ew_filename, dest_lsoas, ttwa_name, mode, marker_style = None,\n",
    "                          plot_save_dir = '', SAVEFIG = False, which_api = 'OTP'):\n",
    "    reload(maputils_pin)\n",
    "    w = 8\n",
    "    fig,ax = plt.subplots(figsize = (w,14))\n",
    "    if not marker_style:\n",
    "        marker_style = dict(color='tab:red', linestyle=':', marker='D',\n",
    "                        markersize=5, markerfacecoloralt='tab:red',\n",
    "                        fillstyle = 'full')\n",
    "    _,_,xs,ys,order = maputils_pin.draw_map_and_landmarks(Bit_df, 'jobs percentage', 'GnBu', 0, 100, None,\n",
    "             None, lsoa_ew_filename, roi_col = 'lsoacd', shp_col = 'lsoa11cd', fig = fig, ax = ax,\n",
    "             params={'SAVEFIG': False}, landmarks = dest_lsoas, marker_style = marker_style, add_names = False,\n",
    "             shp_name_col = 'lsoa11nm')\n",
    "    tmp = ax.set_title('Percentage of reachable jobs for LSOAs in {}'.format(ttwa_name), fontsize = 18)\n",
    "    h = w * (max(ys)-min(ys))/(max(xs)-min(xs))\n",
    "    fig.set_size_inches(w,h, forward=True)\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(os.path.join(plot_save_dir,'reachable_jobs_by_origin_LSOA_{}_{}_{}.png'.format(\n",
    "            ttwa_name,mode,which_api)), bbox_inches = 'tight')\n",
    "    plt.draw()\n",
    "    return xs, ys, order\n",
    "\n",
    "def plot_feasible_edges(xs,ys,order,origin_lsoas,origin_lsoas_coord, durations_matrix):\n",
    "    # first, reshape the bounding box coordinates\n",
    "    xs = np.reshape(np.array(xs),(-1,2))\n",
    "    ys = np.reshape(np.array(ys),(-1,2))\n",
    "    N = 20\n",
    "    # collect the centers from the map data\n",
    "    tmp = [(np.mean(xs[t]),np.mean(ys[t])) for t in range(xs.shape[0])]\n",
    "    centers = []\n",
    "    for lsoa in origin_lsoas:\n",
    "        ix = order.index(lsoa)\n",
    "        centers.append(tmp[ix])\n",
    "    # transform into array\n",
    "    centers_array = np.array(centers)\n",
    "    coords_array = np.array(origin_lsoas_coord)\n",
    "    # remove the mean\n",
    "    centers_array = centers_array - np.mean(centers_array, axis= 0, keepdims = True)\n",
    "    centers_array = centers_array / np.max(centers_array, axis = 0)\n",
    "    coords_array = coords_array - np.mean(coords_array, axis= 0, keepdims = True)\n",
    "    coords_array = coords_array / np.max(coords_array, axis = 0)\n",
    "    fig = plt.figure(figsize = (15,15))\n",
    "    Nx, Ny = durations_matrix.shape\n",
    "    for ix in range(Nx):\n",
    "        for iy in range(ix+1,Ny):\n",
    "            if durations_matrix[ix,iy]<45:\n",
    "                plt.plot([coords_array[ix,0],coords_array[iy,0]],[coords_array[ix,1],coords_array[iy,1]],\n",
    "                         'k',alpha = 0.6)\n",
    "    plt.plot(coords_array[:,0],coords_array[:,1],'o')\n",
    "    tmp = plt.plot(coords_array[:N,0],coords_array[:N,1],'o')\n",
    "    plt.draw()\n",
    "\n",
    "def plot_commute_curve(EofC2_vector, ms, ttwa_name, mode, plot_save_dir = '', SAVEFIG = False, which_api = 'OTP'):\n",
    "    # plot EofC2 vs m\n",
    "    with sns.plotting_context(\"talk\"):\n",
    "        fig, ax = plt.subplots(figsize=(7,4))\n",
    "        sns.lineplot(x = ms*100, y = EofC2_vector*100, color='k', marker = 'o')\n",
    "        sns.lineplot(x = ms*100, y =100 + np.zeros_like(ms), color = [1, 90/255, 0])\n",
    "        plt.xlabel('population threshold (%)')\n",
    "        plt.ylabel('percentage of accessible jobs')\n",
    "        plt.title('TTWA: {}'.format(ttwa_name))\n",
    "        plt.legend(('Actual curve','Ideal curve'), fontsize= 16, frameon = False)\n",
    "        # think about the interpretation again +  check R + \n",
    "    if SAVEFIG:\n",
    "        plt.savefig(os.path.join(plot_save_dir,'ease_of_commute_score2_distribution_{}_{}_{}.png'.format(\n",
    "            ttwa_name,mode,which_api)), bbox_inches = 'tight')\n",
    "    plt.draw()\n",
    "\n",
    "def plot_commute_curve_check(Ajt, jobs_vector_norm, EofC2_vector, ttwa_name, mode,\n",
    "                            plot_save_dir = '', SAVEFIG = False, which_api = 'OTP'):\n",
    "    # only use function to show the rationale behind the commuting curve\n",
    "    # basically, this is only kept for legacy reasons\n",
    "    base = np.sort(Ajt)\n",
    "    ix_sort = np.argsort(Ajt)\n",
    "    jobs_vector_sorted = jobs_vector_norm[ix_sort]\n",
    "    cumulative = np.arange(1,len(Ajt)+1)/len(Ajt)\n",
    "    cumulative2 = np.cumsum(jobs_vector_sorted)\n",
    "    cumulative2 = cumulative2/np.max(cumulative2)\n",
    "    with sns.plotting_context('talk'):\n",
    "        plt.figure(figsize = (7,4))\n",
    "        plt.plot(base*100,cumulative, marker = 'o')\n",
    "        plt.plot(ms*100, 1- EofC2_vector, marker = 'o')\n",
    "        plt.plot(base*100,cumulative2, '--o', markersize = 7)\n",
    "        plt.legend(['ecdf','1 - EofC2','weighted ecdf'])\n",
    "        plt.xlabel('population threshold (%)')\n",
    "        plt.ylabel('probability')\n",
    "        plt.title('TTWA: {}'.format(ttwa_name))\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(os.path.join(plot_save_dir,'ease_of_commute_score2_interpretation_{}_{}_{}.png'.format(\n",
    "            ttwa_name, mode, which_api)), bbox_inches = 'tight')\n",
    "    plt.draw()\n",
    "\n",
    "'''\n",
    "Functions for computing measures (individual TTWAs). For definitions refer to the google doc\n",
    "'''\n",
    "\n",
    "def compute_jobs_and_population(origin_lsoas, lsoa_data, oa_data, \n",
    "                                dest_lsoas, local_lsoa_number_of_jobs):\n",
    "    # TODO: this function might need to be reviewed!!\n",
    "    # assign and multiply by how many people are in each LSOA\n",
    "    population_vector = np.zeros((len(origin_lsoas)))\n",
    "    for ix,lsoa in enumerate(origin_lsoas):\n",
    "        for oa in lsoa_data['oa_list'].loc[lsoa]:\n",
    "            population_vector[ix] += oa_data['residents'].loc[oa]\n",
    "    # normalise so that it sums to 1\n",
    "    population_vector_norm = population_vector / np.sum(population_vector)\n",
    "\n",
    "    # load how many jobs there are in each LSOA\n",
    "    N = len(dest_lsoas)\n",
    "    jobs_vector = np.array(local_lsoa_number_of_jobs['number of jobs'].values)[:N]\n",
    "    #jobs_vector = np.zeros((len(dest_lsoas)))\n",
    "    #for ix,lsoa in enumerate(dest_lsoas):\n",
    "    #    for oa in lsoa_data['oa_list'].loc[lsoa]:\n",
    "    #        jobs_vector[ix] += local_oa_number_of_jobs[oa]\n",
    "    # normalise so that it sums to 1\n",
    "    jobs_vector_norm = jobs_vector / np.sum(jobs_vector)\n",
    "    return population_vector, population_vector_norm, jobs_vector, jobs_vector_norm\n",
    "\n",
    "# compute cumulative accessibility measures\n",
    "def get_cumulative_accessibility_measures(durations_matrix, Th, \n",
    "                            jobs_vector_norm, population_vector_norm, origin_lsoas, dest_lsoas):\n",
    "    # for each origin LSOA, compute how many jobs can be reached within the threshold\n",
    "    Bit = np.sum((durations_matrix<Th) * jobs_vector_norm[:,np.newaxis], axis = 0)*100\n",
    "    # add it to a dataframe\n",
    "    Bit_df = pd.DataFrame(Bit, columns = ['jobs percentage'])\n",
    "    # add the LSOA code to the dataframe\n",
    "    Bit_df = Bit_df.join(pd.DataFrame(origin_lsoas , columns =['lsoacd']))\n",
    "    # for each destination LSOA compute the sum of residents that can reach it within 45 minutes\n",
    "    Ajt = np.sum(population_vector_norm * (durations_matrix < Th), axis = 1)\n",
    "    # now compute the same vector but weighted by the number of jobs at destination\n",
    "    Wjt = Ajt * jobs_vector_norm\n",
    "    EofC1 = np.sum(Wjt)\n",
    "    # compute other aggregated quantities\n",
    "    # ease of commute 1 normalised\n",
    "    EofC1min = np.sum(population_vector_norm[:len(dest_lsoas)] * jobs_vector_norm)\n",
    "    EofC1norm = 1/(1-EofC1min) * (EofC1 - EofC1min)\n",
    "    # ease of commute 2\n",
    "    m = 0.5\n",
    "    EofC2 = np.sum(jobs_vector_norm * ((Ajt - m)>0))\n",
    "    # ease of commute graph\n",
    "    base = np.sort(Ajt)\n",
    "    ix_sort = np.argsort(Ajt)\n",
    "    jobs_vector_sorted = jobs_vector_norm[ix_sort]\n",
    "    cumulative = np.arange(1,len(Ajt)+1)/len(Ajt)\n",
    "    cumulative2 = np.cumsum(jobs_vector_sorted)\n",
    "    cumulative2 = cumulative2/np.max(cumulative2)\n",
    "    # compute the EofC2 vector for relevant threshold values\n",
    "    ms = np.array([0] + [base[0]-0.001] + [t +.001 for t in list(base[0:-1])] + [base[-1]-.001] + [\n",
    "        base[-1]+.001]+ [1])\n",
    "    EofC2_vector = np.sum(jobs_vector_norm[:,np.newaxis] * (np.subtract.outer(Ajt,ms) > 0), axis = 0)\n",
    "    return Bit, Bit_df, Ajt, Wjt, EofC1, EofC1min, EofC1norm, m, EofC2, ms, EofC2_vector\n",
    "\n",
    "# print some relevant information\n",
    "def print_scores_to_file(plot_save_dir, ttwa_formal_name, EofC1, EofC1norm, EofC2, EofC2_vector, m, ms):\n",
    "    with open(os.path.join(plot_save_dir,'out_{}.txt'.format(ttwa_formal_name)), 'w') as f:\n",
    "        print('Overall, {} has an ease of commute score nb. 1 of {:.3f}'.format(ttwa_formal_name,EofC1), file= f)\n",
    "        # the following assumes that the origin LSOAs are ordered so that the first D of them are the destinations\n",
    "        print('Overall, {} has a normalised ease of commute score nb. 1 of {:.3f}'.format(ttwa_formal_name,\n",
    "                                                                            EofC1norm),file= f)\n",
    "        print('Overall, {} has an ease of commute score nb. 2 of {:.1f}%. That is, '.format(\n",
    "            ttwa_formal_name,EofC2*100) + \n",
    "            'this is the percentage of jobs that can be reached by at least {:.0f}% of the population '.format(\n",
    "                100*m)+ \n",
    "            'within the time constraint', file= f)\n",
    "    # Note: the last sentence means that for each of the jobs in that 58.5% I can find a subset of LSOAs from which\n",
    "    # it can be reached within 45 minutes that together account for at least 50% of the population\n",
    "    # Saying that no job can be reached by 80% of the population means that there is no specified job that can be\n",
    "    # reached by 80% of the population in less than 45 minutes.\n",
    "    # get the maximum percentage of population that can access all jobs within the threshold:\n",
    "    max_pop = ms[np.where(EofC2_vector>0.99999)[0][-1]]\n",
    "    # print the minimum population percentage for which the percentage of accessible jobs is lower than 50%:\n",
    "    min_pop = ms[np.where(EofC2_vector<0.5)[0][0]]\n",
    "    with open(os.path.join(plot_save_dir, 'out_{}.txt'.format(ttwa_formal_name)) , 'a') as f:\n",
    "        print('The maximum percentage of population that can access all jobs within the threshold is {:.2f}%'.format(\n",
    "            max_pop*100), file= f)\n",
    "        print('The minimum population percentage for which the percentage of accessible jobs is lower than 50% ' + \n",
    "              'is {:.2f}%'.format(min_pop*100), file= f)\n",
    "    \n",
    "def print_scores(ttwa_formal_name, EofC1, EofC1norm, EofC2, EofC2_vector, m, ms):\n",
    "    # print again here\n",
    "    print('Overall, {} has an ease of commute score nb. 1 of {:.3f}'.format(ttwa_formal_name,EofC1))\n",
    "    print('Overall, {} has a normalised ease of commute score nb. 1 of {:.3f}'.format(ttwa_formal_name,EofC1norm))\n",
    "    print('Overall, {} has an ease of commute score nb. 2 of {:.1f}%. That is, '.format(ttwa_formal_name,EofC2*100)+ \n",
    "          'this is the percentage of jobs that can be reached by at least {:.0f}% of the population '.format(100*m)+ \n",
    "          'within the time constraint')\n",
    "    \n",
    "    # get the maximum percentage of population that can access all jobs within the threshold:\n",
    "    max_pop = ms[np.where(EofC2_vector>0.99999)[0][-1]]\n",
    "    # print the minimum population percentage for which the percentage of accessible jobs is lower than 50%:\n",
    "    min_pop = ms[np.where(EofC2_vector<0.5)[0][0]]\n",
    "    print('The maximum percentage of population that can access all jobs within the threshold is {:.2f}%'.format(\n",
    "        max_pop*100))\n",
    "    print('The minimum population percentage for which the percentage of accessible jobs is lower than 50% ' + \n",
    "          'is {:.2f}%'.format(min_pop*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the threshold\n",
    "Th = {'CAR': 1, 'WALK, TRANSIT': 2}[modes] * median_travel_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "For each TTWA in WM I have the following info arranged in dictionaries\n",
    "with the ttwa codes as keys:\n",
    "(all_durations, all_distances, origin_lsoas_coord, dest_lsoas_coord, \n",
    "origin_lsoas_names, dest_lsoas_names, origin_lsoas, dest_lsoas)\n",
    "I also have local_lsoas_number_of_jobs but the key is the index from the regional_ttwa dataframe\n",
    "'''\n",
    "commute_measures = {}\n",
    "for t,ttwa in enumerate(regional_ttwa.index):\n",
    "    t0 = time.time()\n",
    "    ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "    local_lsoa = lsoa_data[lsoa_data['ttwa']==ttwa_code].join(local_lsoas_number_of_jobs[ttwa]).sort_values(\n",
    "            'number of jobs', ascending = False)\n",
    "    # first compute all the quantities:\n",
    "    # population and jobs vectors\n",
    "    population_vector, population_vector_norm, jobs_vector, jobs_vector_norm = compute_jobs_and_population(\n",
    "                                origin_lsoas[ttwa_code], lsoa_data, oa_data, dest_lsoas[ttwa_code], \n",
    "                                local_lsoas_number_of_jobs[ttwa])\n",
    "    # jobs accessibility measures\n",
    "    Bit,Bit_df,Ajt,Wjt,EofC1,EofC1min,EofC1norm, m, EofC2, ms, EofC2_vector = get_cumulative_accessibility_measures(\n",
    "                            all_durations[ttwa_code], Th, jobs_vector_norm, \n",
    "                            population_vector_norm, origin_lsoas[ttwa_code], dest_lsoas[ttwa_code])\n",
    "    \n",
    "    commute_measures[ttwa_code] = {}\n",
    "    commute_measures[ttwa_code]['name'] = ttwa_formal_names[ttwa]\n",
    "    # store the quantities\n",
    "    for var in ['population_vector', 'population_vector_norm', 'jobs_vector', 'jobs_vector_norm', 'Bit', 'Bit_df',\n",
    "                'Ajt','Wjt','EofC1','EofC1min','EofC1norm', 'm', 'EofC2', 'ms', 'EofC2_vector']:\n",
    "        exec('commute_measures[ttwa_code][var] = ' + var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for t,ttwa in enumerate(regional_ttwa.index):\n",
    "    ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "    local_lsoa = lsoa_data[lsoa_data['ttwa']==ttwa_code].join(local_lsoas_number_of_jobs[ttwa]).sort_values(\n",
    "            'number of jobs', ascending = False)\n",
    "\n",
    "    # plot the heatmap\n",
    "    plot_durations_heatmap(all_durations[ttwa_code], Th, origin_lsoas[ttwa_code], dest_lsoas[ttwa_code],\n",
    "                      ttwa_formal_names[ttwa], lmodes, plot_save_dir = plot_save_dir, SAVEFIG = SAVEFIG)\n",
    "    \n",
    "    # plot population histogram\n",
    "    plot_population_hist(commute_measures[ttwa_code]['population_vector'], ttwa_formal_names[ttwa], \n",
    "                         plot_save_dir = plot_save_dir, SAVEFIG = SAVEFIG)\n",
    "    \n",
    "    # plot jobs vector\n",
    "    plot_jobs_hist(commute_measures[ttwa_code]['jobs_vector'], ttwa_formal_names[ttwa], \n",
    "                    plot_save_dir = plot_save_dir, SAVEFIG = SAVEFIG, t_rotation = 45, nbins = 10)\n",
    "    \n",
    "    # to plot hist of destination accessibility \n",
    "    plot_dest_accessibility_hist(commute_measures[ttwa_code]['Ajt'], ttwa_formal_names[ttwa], lmodes, \n",
    "                    plot_save_dir = plot_save_dir, SAVEFIG = SAVEFIG, t_rotation = 45, nbins = 5)\n",
    "    \n",
    "    # plot accessibility curve\n",
    "    plot_commute_curve(commute_measures[ttwa_code]['EofC2_vector'], commute_measures[ttwa_code]['ms'], \n",
    "                        ttwa_formal_names[ttwa], lmodes, plot_save_dir = plot_save_dir, SAVEFIG = SAVEFIG)\n",
    "print_elapsed(t0, 'plotting all non-map graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "reload(maputils_pin)\n",
    "for t,ttwa in enumerate(regional_ttwa.index):\n",
    "    ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "    local_lsoa = lsoa_data[lsoa_data['ttwa']==ttwa_code].join(local_lsoas_number_of_jobs[ttwa]).sort_values(\n",
    "            'number of jobs', ascending = False)\n",
    "    # plot accessibility on a map\n",
    "    xs, ys, order = plot_accessibility_map(commute_measures[ttwa_code]['Bit_df'], lsoa_ew_filename, \n",
    "                                           dest_lsoas[ttwa_code], \n",
    "                                    ttwa_formal_names[ttwa], lmodes, marker_style = None,\n",
    "                                  plot_save_dir = plot_save_dir, SAVEFIG = SAVEFIG)\n",
    "    \n",
    "    ## plot feasible journeys\n",
    "    #plot_feasible_edges(xs, ys, order, origin_lsoas[ttwa_code], \n",
    "    #                    origin_lsoas_coord[ttwa_code], all_durations[ttwa_code])\n",
    "print_elapsed(t0, 'plotting all maps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the scores\n",
    "for t,ttwa in enumerate(regional_ttwa.index):\n",
    "    ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "    local_lsoa = lsoa_data[lsoa_data['ttwa']==ttwa_code].join(local_lsoas_number_of_jobs[ttwa]).sort_values(\n",
    "            'number of jobs', ascending = False)  \n",
    "    # print some scores\n",
    "    print_scores(ttwa_formal_names[ttwa], commute_measures[ttwa_code]['EofC1'], \n",
    "                 commute_measures[ttwa_code]['EofC1norm'], commute_measures[ttwa_code]['EofC2'], \n",
    "                 commute_measures[ttwa_code]['EofC2_vector'], commute_measures[ttwa_code]['m'], \n",
    "                 commute_measures[ttwa_code]['ms'])\n",
    "    print_scores_to_file(plot_save_dir, ttwa_formal_names[ttwa], commute_measures[ttwa_code]['EofC1'], \n",
    "                 commute_measures[ttwa_code]['EofC1norm'], commute_measures[ttwa_code]['EofC2'], \n",
    "                 commute_measures[ttwa_code]['EofC2_vector'], commute_measures[ttwa_code]['m'], \n",
    "                 commute_measures[ttwa_code]['ms'])\n",
    "    print('-'*117)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now some plots for the entire region\n",
    "# Print out a table with the different EofC scores\n",
    "tmp_df = pd.DataFrame.from_dict(commute_measures, orient= 'index', columns = ['EofC1norm', 'name']).sort_values(\n",
    "    by = 'EofC1norm')\n",
    "print(tmp_df)\n",
    "# now plot this as a bar chart\n",
    "plt.figure(figsize = (10,5))\n",
    "tmp_df['EofC1norm'].plot(kind = 'barh')\n",
    "plt.xlim([0,1])\n",
    "tmp= plt.gca().set_yticklabels(tmp_df['name'], fontsize = 12)\n",
    "plt.xlabel('Jobs accessibility index by {} in {}'.format(lmodes,region_name), fontsize = 14)\n",
    "plt.ylabel('TTWA', fontsize = 14)\n",
    "plt.gca().set_xticklabels(plt.gca().get_xticklabels(), fontsize = 12)\n",
    "for ix,val in enumerate(tmp_df['EofC1norm'].values):\n",
    "    plt.text(val,ix-.15,'{:.3f}'.format(val), fontsize = 12)\n",
    "plt.tight_layout()\n",
    "if SAVEFIG:\n",
    "    plt.savefig(os.path.join(plot_save_dir, 'jobs_accessibility_index_by_{}_in_{}'.format(\n",
    "        lmodes,region_name.replace(' ',''))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot a graph with the EofC curves for all the TTWAs + the ideal curve\n",
    "reload(utils_pin)\n",
    "markers_list = ['o','v','^','<','>','s','d','x','+','*','h','o']\n",
    "from utils_pin import nesta_colours, nesta_colours_combos\n",
    "def plot_multiple_commute_curves(commute_measures_dict, area_name, mode, plot_save_dir = '', \n",
    "                                SAVEFIG = False, which_api = 'OTP'):\n",
    "    assert(isinstance(commute_measures_dict,dict))\n",
    "    ks = commute_measures_dict.keys()\n",
    "    ic = -1\n",
    "    ideal_curve_colour = 2\n",
    "    # plot EofC2 vs m\n",
    "    with sns.plotting_context(\"talk\"):\n",
    "        fig, ax = plt.subplots(figsize=(14,5))\n",
    "        for key in ks:\n",
    "            ic += 1\n",
    "            #if ic == ideal_curve_colour:\n",
    "            #    ic+=1\n",
    "            plt.plot(commute_measures_dict[key]['ms']*100, \n",
    "                         commute_measures_dict[key]['EofC2_vector']*100, color=nesta_colours[ic], \n",
    "                         marker = markers_list[ic], label = commute_measures_dict[key]['name'])\n",
    "        plt.plot(ms*100, 100 + np.zeros_like(ms), '-.o', markerfacecolor = 'none', \n",
    "                 color = nesta_colours[ideal_curve_colour], label = 'Ideal curve')\n",
    "        plt.xlabel('population threshold (%)')\n",
    "        plt.ylabel('percentage of accessible jobs')\n",
    "        plt.title('Area: {}'.format(area_name))\n",
    "        plt.legend(fontsize= 16, frameon = False, bbox_to_anchor=(1.1, 1.05))\n",
    "        # think about the interpretation again +  check R + \n",
    "        plt.tight_layout()\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(os.path.join(plot_save_dir,'ease_of_commute_score2_distribution_{}_{}_{}.png'.format(\n",
    "            area_name.replace(' ',''),mode,which_api)), bbox_inches = 'tight')\n",
    "    plt.draw()\n",
    "    \n",
    "plot_multiple_commute_curves(commute_measures, region_name, lmodes,plot_save_dir = plot_save_dir,\n",
    "                            SAVEFIG = SAVEFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average commuting distance\n",
    "groups = oa_data.groupby('ttwa')\n",
    "M_dist_ttwa = np.zeros(len(regional_ttwa.index))\n",
    "M_dist_ttwa2 = {}\n",
    "for jj,ttwa in enumerate(regional_ttwa.index):\n",
    "    ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "    # get the group corresponding to this TTWA\n",
    "    group = groups.get_group(ttwa_code)\n",
    "    # collect the mean distances to work\n",
    "    dks = np.array(group['Mean distance to work (overall)'])\n",
    "    # go through all the occupations\n",
    "    M_dist_ttwa[jj] = np.nanmean(dks)\n",
    "    M_dist_ttwa2[ttwa_code] = np.nanmean(dks)\n",
    "print(M_dist_ttwa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange the data into a dataframe\n",
    "print(commute_measures['E30000271'].keys())\n",
    "commute_df = pd.DataFrame.from_dict(commute_measures, orient = 'index', columns = ['name', 'EofC1norm', \n",
    "                                                                                'ms', 'EofC2_vector'])\n",
    "commute_df = commute_df.join(pd.DataFrame.from_dict(flying_crow, orient = 'index', \n",
    "                                                    columns = ['all geographical distances'])).join(\n",
    "                        pd.DataFrame.from_dict(M_dist_ttwa2, orient = 'index', columns = ['commute distance'])).join(\n",
    "                        pd.DataFrame.from_dict(avg_ons_times, orient = 'index', columns = ['avg ons times']))\n",
    "commute_df['geographical distance'] = commute_df['all geographical distances'].map(np.mean)\n",
    "print(commute_df.head(n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_list = ['o','v','^','<','>','s','d','x','+','*','h','o']\n",
    "fig = plt.figure(figsize = (20,6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "print(ax1)\n",
    "with sns.plotting_context('talk'):\n",
    "    g = sns.regplot(x = 'geographical distance', y = 'EofC1norm', data = commute_df, ax = ax1)\n",
    "    for t,ttwa in enumerate(regional_ttwa.index):\n",
    "        ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "    #    plt.plot(flying_crow[ttwa_code].mean(),commute_measures[ttwa_code]['EofC1norm'],'o', \n",
    "    #             color = nesta_colours[2])\n",
    "        if commute_measures[ttwa_code]['name'] in ['Coventry','Hereford','Leamington Spa']:\n",
    "            plt.text(flying_crow[ttwa_code].mean()+.1,commute_measures[ttwa_code]['EofC1norm']-.03,\n",
    "                 commute_measures[ttwa_code]['name'])\n",
    "        elif 'Wolverhampton' in commute_measures[ttwa_code]['name']:\n",
    "            plt.text(flying_crow[ttwa_code].mean()-2,commute_measures[ttwa_code]['EofC1norm']+.02,\n",
    "                 commute_measures[ttwa_code]['name'])\n",
    "        elif commute_measures[ttwa_code]['name'] in ['Stoke-on-Trent', 'Worcester and Kidderminster']:\n",
    "            plt.text(flying_crow[ttwa_code].mean()+.1,commute_measures[ttwa_code]['EofC1norm']+.02,\n",
    "                 commute_measures[ttwa_code]['name'])\n",
    "        else:\n",
    "            plt.text(flying_crow[ttwa_code].mean()+.1,commute_measures[ttwa_code]['EofC1norm']+.01,\n",
    "                 commute_measures[ttwa_code]['name'])\n",
    "    plt.xlabel('average geographical distance (miles)', fontsize = 18)\n",
    "    plt.ylabel('jobs accessibility index', fontsize = 18)\n",
    "    ax1.tick_params(axis = 'both', labelsize = 18)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    g = sns.regplot(x = 'commute distance', y = 'EofC1norm', data = commute_df, ax = ax2)\n",
    "    for t,ttwa in enumerate(regional_ttwa.index):\n",
    "        ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "        #plt.plot(M_dist_ttwa[t],commute_measures[ttwa_code]['EofC1norm'],'o', color = nesta_colours[1])\n",
    "        if commute_measures[ttwa_code]['name'] in ['Coventry','Hereford','Leamington Spa']:\n",
    "            plt.text(M_dist_ttwa[t]+.1,commute_measures[ttwa_code]['EofC1norm']-.02,\n",
    "                 commute_measures[ttwa_code]['name'])\n",
    "        elif 'Wolverhampton' in commute_measures[ttwa_code]['name']:\n",
    "            plt.text(M_dist_ttwa[t]-1,commute_measures[ttwa_code]['EofC1norm']+.01,\n",
    "                 commute_measures[ttwa_code]['name'])\n",
    "        elif commute_measures[ttwa_code]['name'] in ['Stoke-on-Trent', 'Worcester and Kidderminster']:\n",
    "            plt.text(M_dist_ttwa[t]+.1,commute_measures[ttwa_code]['EofC1norm']+.01,\n",
    "                 commute_measures[ttwa_code]['name'])\n",
    "        else:\n",
    "            plt.text(M_dist_ttwa[t]+.1,commute_measures[ttwa_code]['EofC1norm']+.005,\n",
    "                 commute_measures[ttwa_code]['name'])\n",
    "        plt.xlabel('average commuting distance (miles)', fontsize = 18)\n",
    "        plt.ylabel('jobs accessibility index', fontsize = 18)\n",
    "    ax2.tick_params(axis = 'both', labelsize = 18)\n",
    "plt.tight_layout()\n",
    "# save if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot accessibility index versus average commuting times from ons\n",
    "fig= plt.figure(figsize = (9,5))\n",
    "ax= fig.gca()\n",
    "with sns.plotting_context('talk'):\n",
    "    g = sns.regplot(x = 'avg ons times', y = 'EofC1norm', data = commute_df, ax = ax)\n",
    "    for t,ttwa in enumerate(regional_ttwa.index):\n",
    "        ttwa_code = regional_ttwa['ttwa11cd'].loc[ttwa]\n",
    "        #plt.plot(M_dist_ttwa[t],commute_measures[ttwa_code]['EofC1norm'],'o', color = nesta_colours[1])\n",
    "        if commute_measures[ttwa_code]['name'] in ['Coventry','Hereford']:\n",
    "            plt.text(avg_ons_times[ttwa_code]-1,commute_measures[ttwa_code]['EofC1norm']-.04,\n",
    "                 commute_measures[ttwa_code]['name'], fontsize = 16)\n",
    "        elif 'Wolverhampton' in commute_measures[ttwa_code]['name']:\n",
    "            plt.text(avg_ons_times[ttwa_code]+.2,commute_measures[ttwa_code]['EofC1norm']-.02,\n",
    "                 commute_measures[ttwa_code]['name'], fontsize = 16)\n",
    "        elif commute_measures[ttwa_code]['name'] in ['Stoke-on-Trent', 'Worcester and Kidderminster']:\n",
    "            plt.text(avg_ons_times[ttwa_code]-2,commute_measures[ttwa_code]['EofC1norm']+.025,\n",
    "                 commute_measures[ttwa_code]['name'], fontsize = 16)\n",
    "        else:\n",
    "            plt.text(avg_ons_times[ttwa_code]+.1,commute_measures[ttwa_code]['EofC1norm']+.005,\n",
    "                 commute_measures[ttwa_code]['name'], fontsize = 16)\n",
    "        plt.xlabel('average commuting distance (miles)', fontsize = 18)\n",
    "        plt.ylabel('jobs accessibility index', fontsize = 18)\n",
    "    ax.tick_params(axis = 'both', labelsize = 18)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possibly, plot accessibility index against employment rate and claimant count (as for distances), maybe size too?\n",
    "# first extract relevant TTWAs\n",
    "ttwa_data['relevant'] = ttwa_data['ttwa11cd'].map(lambda x: x in list(commute_measures.keys()))\n",
    "ttwa_data_red = ttwa_data[ttwa_data['relevant']]\n",
    "ttwa_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in ['Supply-side self-containment', 'Demand-side self containment','Employment rate',\n",
    "            'Male employment rate', 'Female employment rate','Job-seeking economically inactive', 'Claimant count',\n",
    "           'Employed in SOC code 1', 'Employed in SOC code 2',\n",
    "       'Employed in SOC code 5', 'Employed in SOC code 6',\n",
    "       'Employed in SOC code 8', 'Employed in SOC code 9']:\n",
    "    plt.figure() \n",
    "    plt.scatter(commute_df['EofC1norm'],ttwa_data_red[col])\n",
    "    try:\n",
    "        rho = np.corrcoef(commute_df['EofC1norm'].values,ttwa_data_red[col].values)[0,1]\n",
    "    except:\n",
    "        rho = np.corrcoef(commute_df['EofC1norm'].values,ttwa_data_red[col].values.astype(np.float32))[0,1]\n",
    "    #    print(col,ttwa_data_red[col].values)\n",
    "    plt.xlabel('accessibility index. rho={}'.format(np.around(rho,3)))\n",
    "    plt.ylabel(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
