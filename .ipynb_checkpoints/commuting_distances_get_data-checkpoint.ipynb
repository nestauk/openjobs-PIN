{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commuting distance per occupation\n",
    "This notebook is to compute average distance travelled per occupation across different travel to work areas (TTWA). For each output area (OA), or lower super output area (LSOA) in a TTWA, it calls the lmiforall API that returns data on commuting distance and occupation breakdown from the 2011 census.\n",
    "\n",
    "The centroids for each OA, or LSOA, are computed in another script (get_oa_lsoa_centroids) using the ONS postcode directory (February 2019). The same dataset also offers a lookup between OAs and TTWAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "# set the default to darkgrid\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant folders\n",
    "folder1= '/Users/stefgarasto/Local-Data/'\n",
    "folder2 = '/Users/stefgarasto/Google Drive/Documents/data/'\n",
    "folder3 = '/Users/stefgarasto/Google Drive/Documents/results/'\n",
    "folder4 = '/Users/stefgarasto/Google Drive/Documents/data/ONS/derivative-data/'\n",
    "folder5 = folder2 + 'ONS/Travel_to_Work_Areas_2011_guidance_and_information_V4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant files\n",
    "ons_pc_file = folder2 + 'ONS/ONS-Postcode-Directory-Latest-Centroids.csv'\n",
    "pop_density_file = folder2 + 'ONS/population_density1.csv'\n",
    "ttwa_file = folder2 + 'ONS/Travel_to_Work_Areas_December_2011_Boundaries.csv'\n",
    "ttwa_info11_file = folder5 + 'TTWA-summary-statistics-2011.xls'\n",
    "ttwa_info16_file = folder5 + 'TTWA-info-2016.xls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, load the list of all TTWA\n",
    "ttwa_data = pd.read_csv(ttwa_file)\n",
    "# first column is ttwa codes, second column is ttwa names\n",
    "ttwa_info11 = pd.read_excel(ttwa_info11_file)\n",
    "ttwa_info16 = pd.read_excel(ttwa_info16_file)\n",
    "#print(ttwa_info11.tail(n=3))\n",
    "#print(ttwa_info16.tail(n=3))\n",
    "\n",
    "# get small TTWAs\n",
    "small_ttwas = list(ttwa_info11['ttwa11cd'][ttwa_info11['LSOAs']<40])\n",
    "print('There are {} TTWAs with less than 40 LSOAs.'.format(len(small_ttwas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the extracted dictionaries of OA centroids\n",
    "loadOA = True\n",
    "loadLSOA = False\n",
    "oa_path = folder4 + 'oa_centroids_dictionary.pickle'\n",
    "lsoa_path = folder4 + 'lsoa_centroids_dictionary.pickle'\n",
    "exists = os.path.isfile(oa_path)\n",
    "if exists and loadOA:\n",
    "    print('Loadin the OA data')\n",
    "    oa_data = pd.read_pickle(oa_path)\n",
    "\n",
    "exists = os.path.isfile(lsoa_path)\n",
    "if exists and loadLSOA:\n",
    "    print('Loading the LSOA data')\n",
    "    lsoa_data = pd.read_pickle(lsoa_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file where I'm storing all the information\n",
    "save_oa_file = folder3 + 'PIN/oa_distances_and_occupations_v2.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APICALL = False\n",
    "if APICALL:\n",
    "    # for each OA, call the LMIforALL API:\n",
    "    oa_distances2 = {}\n",
    "    oa_occupations2 = {}\n",
    "    oa_residents2 = {}\n",
    "\n",
    "    # it will take multiple hours to gather them all. So, need to create breakpoints\n",
    "    # if file exists load it and start from where it left off\n",
    "    exists = os.path.isfile(save_oa_file)\n",
    "    start_oa = 0\n",
    "    socGroups = {}\n",
    "\n",
    "    #TODO: actually, there is no need to have the entire pre-stored list while making the new calls, only need to join the\n",
    "    # old and new dictionaries in the end.\n",
    "    if exists:\n",
    "        print('Loading already downloaded OAs')\n",
    "        with open(save_oa_file,'rb') as f:\n",
    "            # I only need to load the indices where we left off last time\n",
    "            oa_data,_,_,_,socGroups,_,start_oa = pickle.load(f)\n",
    "        start_oa += 1\n",
    "    end_oa = min(start_oa + 7000, len(oa_data.index))\n",
    "    print(start_oa, end_oa)\n",
    "    if end_oa - start_oa<1:\n",
    "        stop\n",
    "    t0 = time.time()\n",
    "    # loop through all OAs and call the LMIforALL API\n",
    "    N = 999\n",
    "    missing_oas = [[], []]\n",
    "    for ii,oa in enumerate(oa_data.index[start_oa:end_oa]):\n",
    "        if oa[0] in ['L', 'M']:\n",
    "            # exclude these two OAs. They are channel islands and isle of Man, respectively, but they seem to have \n",
    "            # wrong latitude and longitude. I'll check them out better later\n",
    "            continue\n",
    "        # mean distances\n",
    "        urlname = 'http://api.lmiforall.org.uk/api/v1/census/distance?area={:6f}%2C{:6f}'.format(\n",
    "            oa_data.loc[oa]['lat'],oa_data.loc[oa]['long'])\n",
    "        out = requests.get(urlname).json()\n",
    "        if 'distances' in out:\n",
    "            out = out['distances']\n",
    "        else:\n",
    "            print(oa, out)\n",
    "            missing_oas[0].append(oa)\n",
    "            missing_oas[1].append(out)\n",
    "            continue\n",
    "        tmp = {}\n",
    "        # out is now a list of dictionaries, each with a code, a description and a value\n",
    "        for itmp in out:\n",
    "            # saving them this way should make it easy to then convert into a dataframe\n",
    "            tmp[itmp['description']] = itmp['value']\n",
    "            tmp['code'] = itmp['code']\n",
    "        oa_distances2[oa] = tmp\n",
    "        # occupation breakdown\n",
    "        urlname = 'http://api.lmiforall.org.uk/api/v1/census/resident_occupations?area={:6f}%2C{:6f}'.format(\n",
    "            oa_data.loc[oa]['lat'],oa_data.loc[oa]['long'])\n",
    "        out = requests.get(urlname).json()\n",
    "        oa_residents2[oa] = out['totalResidents'] # this one is just a number, so it will be easily convertable\n",
    "        # the following is again a list of dictionaries, with socGroudp, description, value and percentage\n",
    "        out = out['residentOccupations']\n",
    "        tmp = {}\n",
    "        for itmp in out:\n",
    "            # use the socGroup as the key (adding value or pecentage), so that then each SOC will become a column\n",
    "            tmp[itmp['socGroup']+'_value'] = itmp['value']\n",
    "            tmp[itmp['socGroup']+'_percentage'] = itmp['percentage']\n",
    "            # at the same time, keep a list of names associated with socgroups\n",
    "            socGroups[itmp['socGroup']] = itmp['description']\n",
    "        oa_occupations2[oa] = tmp\n",
    "        if ii%1000 == N:\n",
    "            print('Done with the last {} OAs. It took {:4f} s'.format(N+1, time.time() - t0))\n",
    "            t0 = time.time()\n",
    "\n",
    "    t0 = time.time()\n",
    "    # reload the  previously filled dictionary\n",
    "    with open(save_oa_file, 'rb') as f:\n",
    "        _,oa_distances,oa_occupations,oa_residents,_,_,_ = pickle.load(f)\n",
    "    N = len(oa_distances)\n",
    "\n",
    "    # update the dictionaries\n",
    "    oa_distances.update(oa_distances2)\n",
    "    oa_residents.update(oa_residents2)\n",
    "    oa_occupations.update(oa_occupations2)\n",
    "\n",
    "    with open(save_oa_file, 'wb') as f:\n",
    "        pickle.dump((oa_data,oa_distances,oa_occupations,oa_residents,socGroups,start_oa,start_oa+ii),f)\n",
    "\n",
    "    oa_distances = None\n",
    "    oa_residents = None\n",
    "    oa_occupations = None\n",
    "    print('Done. Time spent saving: {:2f} s'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some OAs that I am missing because the API returns that their LONG/LAT is outside of the UK. \n",
    "# The longitude/latitude pair is computed as the average across postcodes belonging to them\n",
    "# To get the data for these OAs, I call the API using one of the postcodes in them \n",
    "# (the first that the ONS postcode directory returns)\n",
    "\n",
    "GETMISSING = False\n",
    "if GETMISSING:\n",
    "    # I am hardcoding these missing OAs for future reference\n",
    "    missing_oas = ['S00128975', 'S00129086', 'S00129114', 'S00129074', 'S00129108', 'S00129102', 'S00129030',\n",
    "                   'S00129125', 'S00129014', 'S00128998', 'S00128971', 'S00129038', 'S00129033', 'S00129072', \n",
    "                   'S00129107', 'S00128967', 'S00128952', 'S00128970', 'S00129092', 'S00128995', 'S00129121', \n",
    "                   'S00129132', 'S00129005', 'S00129105', 'S00129113', 'S00129133', 'S00128963', 'S00129085', \n",
    "                   'S00129139', 'S00128953', 'S00129058', 'S00128942', 'S00129046', 'S00129009', 'S00129048', \n",
    "                   'S00129035', 'S00129004', 'S00129082', 'S00129076', 'S00129028', 'S00129036', 'S00129041',\n",
    "                   'S00129068', 'S00128951', 'S00129138', 'S00129078', 'S00129116', 'S00128943', 'S00129131',\n",
    "                   'S00129119', 'S00128947', 'S00128976', 'L99999999', 'S00129003', 'S00128977', 'S00129090',\n",
    "                   'S00129054', 'S00129011', 'S00128962', 'S00128944', 'S00129126', 'S00129002', 'S00128991',\n",
    "                   'S00128985', 'S00129053', 'S00129129', 'S00129052', 'S00128973', 'S00129112', 'S00128996', \n",
    "                   'S00129106', 'S00129044', 'S00129043', 'S00128972', 'S00129101', 'S00129115', 'S00129049', \n",
    "                   'S00129034', 'S00129088', 'S00129070', 'S00129075', 'S00129083', 'S00129127', 'S00129095', \n",
    "                   'S00129012', 'S00128948', 'S00129061', 'S00129040', 'S00129071', 'S00128997', 'S00129006', \n",
    "                   'S00129089', 'S00129077', 'S00128988', 'S00129032', 'S00129099', 'S00129141', 'S00128946', \n",
    "                   'S00129140', 'S00129010', 'S00129000', 'S00129122', 'S00129021', 'S00128956', 'S00129007', \n",
    "                   'S00128999', 'S00128969', 'S00128945', 'S00129120', 'S00128974', 'S00129008', 'S00129103',\n",
    "                   'S00129094', 'S00128992', 'S00128986', 'S00129135', 'S00129136', 'S00129020', 'S00129123',\n",
    "                   'S00129042', 'S00129109', 'S00129059', 'M99999999', 'S00128954', 'S00129098', 'S00128987', \n",
    "                   'S00128955', 'S00129073', 'S00129050', 'S00129093', 'S00128981', 'S00129104', 'S00129100', \n",
    "                   'S00129065', 'S00129001', 'S00128968', 'S00128966', 'S00129128', 'S00128978', 'S00129026', \n",
    "                   'S00129062', 'S00129066', 'S00129047', 'S00129081', 'S00128958', 'S00129060', 'S00129031', \n",
    "                   'S00128980', 'S00128949', 'S00128964', 'S00129019', 'S00129080', 'S00129015', 'S00128989', \n",
    "                   'S00129013', 'S00129029', 'S00129045', 'S00128957', 'S00129057', 'S00128979', 'S00129079', \n",
    "                   'S00129063', 'S00129069', 'S00128982', 'S00129118', 'S00128961', 'S00129039', 'S00129022', \n",
    "                   'S00129111', 'S00128950', 'S00129110', 'S00128984', 'S00129097', 'S00129023', 'S00129016',\n",
    "                   'S00129024', 'S00128993', 'S00129018', 'S00129091', 'S00129017', 'S00128983', 'S00128990', \n",
    "                   'S00128994', 'S00128965', 'S00129134', 'S00129027', 'S00129096', 'S00128959', 'S00128960', \n",
    "                   'S00129087', 'S00129051', 'S00129055', 'S00129084', 'S00129117', 'S00129056', 'S00129037', \n",
    "                   'S00129067', 'S00129064', 'S00129130', 'S00129137', 'S00129124', 'S00129025']\n",
    "\n",
    "    print('Number of problematic OAs: ', len(missing_oas))\n",
    "    # First, I need to load the ONS postcodes directory and group them by OAs\n",
    "    ons_data = pd.read_csv(ons_pc_file)\n",
    "    groups = ons_data.groupby('oa11')\n",
    "\n",
    "    # get the postcode list for the missing OAs\n",
    "    missing_oas_pcd = []\n",
    "    for missing_oa in missing_oas:\n",
    "        group = groups.get_group(missing_oa)\n",
    "        missing_oas_pcd.append(list(group['pcd']))\n",
    "\n",
    "# now call the LMI API using the first postcode\n",
    "APICALL_missed = False\n",
    "if APICALL_missed and GETMISSING:\n",
    "    # for each OA, call the LMIforALL API:\n",
    "    oa_distances2 = {}\n",
    "    oa_occupations2 = {}\n",
    "    oa_residents2 = {}\n",
    "\n",
    "    # actually, there is no need to have the entire pre-stored list while making the new calls, only need to join \n",
    "    # the old and new dictionaries in the end.\n",
    "    t0 = time.time()\n",
    "    # loop through all OAs and call the LMIforALL API\n",
    "    N = 49\n",
    "    failed_oas = [[], []]\n",
    "    for ii,oa in enumerate(missing_oas):\n",
    "        # still skip Channel Island and Isle of Man\n",
    "        if oa[0] in ['L', 'M']:\n",
    "            continue\n",
    "        # mean distances\n",
    "        if oa == 'S00128975':\n",
    "            ipcd = 1\n",
    "        else:\n",
    "            ipcd = 0\n",
    "        urlname = 'http://api.lmiforall.org.uk/api/v1/census/distance?area={}'.format(\n",
    "            missing_oas_pcd[ii][ipcd].replace(' ',''))\n",
    "        out = requests.get(urlname).json()\n",
    "        if 'distances' in out:\n",
    "            out = out['distances']\n",
    "        else:\n",
    "            print(oa, out)\n",
    "            failed_oas[0].append(oa)\n",
    "            failed_oas[1].append(out)\n",
    "            continue\n",
    "        tmp = {}\n",
    "        # out is now a list of dictionaries, each with a code, a description and a value\n",
    "        for itmp in out:\n",
    "            # saving them this way should make it easy to then convert into a dataframe\n",
    "            tmp[itmp['description']] = itmp['value']\n",
    "            tmp['code'] = itmp['code']\n",
    "        oa_distances2[oa] = tmp\n",
    "        # occupation breakdown\n",
    "        urlname = 'http://api.lmiforall.org.uk/api/v1/census/resident_occupations?area={}'.format(\n",
    "            missing_oas_pcd[ii][ipcd].replace(' ',''))\n",
    "        out = requests.get(urlname).json()\n",
    "        oa_residents2[oa] = out['totalResidents'] # this one is just a number, so it will be easily convertable\n",
    "        # the following is again a list of dictionaries, with socGroudp, description, value and percentage\n",
    "        out = out['residentOccupations']\n",
    "        tmp = {}\n",
    "        for itmp in out:\n",
    "            # use the socGroup as the key (adding value or pecentage), so that then each SOC will become a column\n",
    "            tmp[itmp['socGroup']+'_value'] = itmp['value']\n",
    "            tmp[itmp['socGroup']+'_percentage'] = itmp['percentage']\n",
    "            # at the same time, keep a list of names associated with socgroups\n",
    "            socGroups[itmp['socGroup']] = itmp['description']\n",
    "        oa_occupations2[oa] = tmp\n",
    "        if ii%(N+1) == N:\n",
    "            print('Done with the last {} OAs. It took {:4f} s'.format(N+1, time.time() - t0))\n",
    "            t0 = time.time()\n",
    "            \n",
    "    # now reload the  previously filled dictionary\n",
    "    t0 = time.time()\n",
    "    with open(save_oa_file, 'rb') as f:\n",
    "        _,oa_distances,oa_occupations,oa_residents,socGroups,_,_ = pickle.load(f)\n",
    "    N = len(oa_distances)\n",
    "\n",
    "    # update the dictionaries with the missing OAs\n",
    "    oa_distances.update(oa_distances2)\n",
    "    oa_residents.update(oa_residents2)\n",
    "    oa_occupations.update(oa_occupations2)\n",
    "\n",
    "    # save it\n",
    "    with open(save_oa_file, 'wb') as f:\n",
    "        pickle.dump((oa_data,oa_distances,oa_occupations,oa_residents,socGroups,0,len(oa_data)),f)\n",
    "\n",
    "    oa_distances = None\n",
    "    oa_residents = None\n",
    "    oa_occupations = None\n",
    "    print('Done. Time spent saving: {:2f} s'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally, we would also collect data about jobs breakdown by OA, that is the number of jobs in that area, total and\n",
    "# split by SOC codes: TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Check memory usage of all variables\n",
    "'''\n",
    "\n",
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') \n",
    "        and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
