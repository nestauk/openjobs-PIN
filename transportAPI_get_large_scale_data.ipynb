{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransportAPI script: large scale data collection\n",
    "\n",
    "TODO: description\n",
    "\n",
    "General TODOs: \n",
    "- work with a list of TTWAs, \n",
    "- load the TTWAs automatically, \n",
    "- reduce the information to be collected from LMI for ALL on the fly: what else do I need to collect offline?\n",
    "- can I monitor usage?\n",
    "- check \"the population weighted centroids and the jobs weighted centroids\"\n",
    "- balance between lots of savings and computational time\n",
    "\n",
    "[what's the required output data? in case we use ONS software]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant folders\n",
    "folder1= '/Users/stefgarasto/Local-Data/'\n",
    "folder2 = '/Users/stefgarasto/Google Drive/Documents/data/'\n",
    "folder3 = '/Users/stefgarasto/Google Drive/Documents/results/'\n",
    "folder4 = '/Users/stefgarasto/Google Drive/Documents/data/ONS/derivative-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant files\n",
    "ons_pc_file = folder2 + 'ONS/ONS-Postcode-Directory-Latest-Centroids.csv'\n",
    "pop_density_file = folder2 + 'ONS/population_density1.csv'\n",
    "ttwa_file = folder2 + 'ONS/Travel_to_Work_Areas_December_2011_Boundaries.csv'\n",
    "target_ttwa_file = folder2 + 'ONS/PIN_Target_Travel_to_Work_Areas.csv'\n",
    "bres_lsoa_file = folder2 + 'ONS/BRES_employment_lsoa_2011_total.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, load the list of all TTWA\n",
    "ttwa_data = pd.read_csv(ttwa_file)\n",
    "# now load which TTWAs you want to collect data for\n",
    "##target_ttwa = pd.read_csv(target_ttwa_file)\n",
    "# for both dataframes first column is ttwa codes, second column is ttwa names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the OA data\n",
      "Loading the LSOA data\n"
     ]
    }
   ],
   "source": [
    "# load the extracted dictionaries of OA centroids\n",
    "loadOA = True\n",
    "loadLSOA = True\n",
    "oa_centroid_path = folder4 + 'oa_centroids_dictionary.pickle'\n",
    "lsoa_centroid_path = folder4 + 'lsoa_centroids_dictionary.pickle'\n",
    "exists = os.path.isfile(oa_centroid_path)\n",
    "if exists and loadOA:\n",
    "    print('Loading the OA data')\n",
    "    oa_data = pd.read_pickle(oa_centroid_path)\n",
    "else:\n",
    "    print('File not found or not requested')\n",
    "exists = os.path.isfile(lsoa_centroid_path)\n",
    "if exists and loadLSOA:\n",
    "    print('Loading the LSOA data')\n",
    "    lsoa_data = pd.read_pickle(lsoa_centroid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                lat      long       ttwa  \\\n",
      "lsoa11                                     \n",
      "95AA01S1  54.652070 -6.212145  N12000002   \n",
      "95AA01S2  54.637873 -6.240701  N12000002   \n",
      "95AA01S3  54.687938 -6.183465  N12000002   \n",
      "95AA02W1  54.709002 -6.221532  N12000002   \n",
      "95AA03W1  54.706868 -6.183428  N12000002   \n",
      "\n",
      "                                                    oa_list  \n",
      "lsoa11                                                       \n",
      "95AA01S1                                        [N00000001]  \n",
      "95AA01S2  [N00000002, N00000003, N00000004, N00000005, N...  \n",
      "95AA01S3       [N00000007, N00000008, N00000009, N00000010]  \n",
      "95AA02W1  [N00000011, N00000012, N00000013, N00000014, N...  \n",
      "95AA03W1  [N00000017, N00000018, N00000019, N00000020, N...  \n",
      "                 lat      long       ttwa     lsoa11\n",
      "oa11                                                \n",
      "E00000001  51.520345 -0.094809  E30000234  E01000001\n",
      "E00000003  51.519846 -0.096589  E30000234  E01000001\n",
      "E00000005  51.519273 -0.096724  E30000234  E01000001\n",
      "E00000007  51.516421 -0.096969  E30000234  E01000001\n",
      "E00000010  51.522483 -0.097389  E30000234  E01000003\n"
     ]
    }
   ],
   "source": [
    "print(lsoa_data.head())\n",
    "print(oa_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "- After getting all the OAs in that area, I will get the job breakdown for each OA\n",
    "- Then I sum the number of jobs across all OAs in the same LSOA\n",
    "- Then I order the LSOAs by absolute number of jobs, in descending order\n",
    "- Starting from the LSOA with the most jobs, I call the transport API to compute journey time from each other LSOA to one destination LSOA. The latter is selected going down the list of LSOA ordered by number of jobs, until I  finish the free calls available this month with transport API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 619 OAs in the cheltenham TTWA\n",
      "There are 113 OAs in the cheltenham TTWA\n"
     ]
    }
   ],
   "source": [
    "# get all the OAs and LSOAs in the Stafford TTWA and call the lmi for all to get the job breakdown\n",
    "t0 = time.time()\n",
    "local_ttwa = 'E30000189' #'E30000271' #271 is stafford, 189 is Chelthenham\n",
    "local_oa = oa_data[oa_data['ttwa'] == local_ttwa]\n",
    "local_lsoa = lsoa_data[lsoa_data['ttwa'] == local_ttwa]\n",
    "ttwa_formal_name = ttwa_data['ttwa11nm'][ttwa_data['ttwa11cd']==local_ttwa].values[0]\n",
    "ttwa_name = ttwa_formal_name.lower().replace(' ','-') #'cheltenham'\n",
    "print('There are {} OAs in the {} TTWA'.format(len(local_oa), ttwa_name))\n",
    "print('There are {} LSOAs in the {} TTWA'.format(len(local_lsoa), ttwa_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, in 46.242422s\n"
     ]
    }
   ],
   "source": [
    "# call lmi for all for the job breakdown\n",
    "# TODO: Ideally, this would have been collected and saved somewhere already\n",
    "CALL_LMI = True\n",
    "if CALL_LMI:\n",
    "    local_oa_number_of_jobs = {}\n",
    "    local_oa_jobs_breakdown = {}\n",
    "    local_jobs_socGroups = {}\n",
    "    local_oa_population = {}\n",
    "    for ii,oa in enumerate(local_oa.index):\n",
    "        urlname = 'http://api.lmiforall.org.uk/api/v1/census/jobs_breakdown?area={:6f}%2C{:6f}'.format(\n",
    "            local_oa.loc[oa]['lat'],local_oa.loc[oa]['long'])\n",
    "        out = requests.get(urlname).json()\n",
    "        local_oa_number_of_jobs[oa] = out['totalJobs']\n",
    "        out = out['jobsBreakdown']\n",
    "        tmp = {}\n",
    "        for itmp in out:\n",
    "            # use the socGroup as the key (adding value or pecentage), so that then each SOC will become a column\n",
    "            tmp[itmp['socGroup']+'_value'] = itmp['value']\n",
    "            tmp[itmp['socGroup']+'_percentage'] = itmp['percentage']\n",
    "            # at the same time, keep a list of names associated with socgroups\n",
    "            local_jobs_socGroups[itmp['socGroup']] = itmp['description']\n",
    "        local_oa_jobs_breakdown[oa] = tmp\n",
    "        # get also the number of residents (population based estimate)\n",
    "        urlname = 'http://api.lmiforall.org.uk/api/v1/census/resident_occupations?area={:6f}%2C{:6f}'.format(\n",
    "            local_oa.loc[oa]['lat'],local_oa.loc[oa]['long'])\n",
    "        out = requests.get(urlname).json()\n",
    "        local_oa_population[oa] = out['totalResidents']\n",
    "else:\n",
    "    # load the saved data\n",
    "    pass\n",
    "print('Done, in {:4f}s'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompute the population weighted centroids and the jobs weighted centroids\n",
    "pop_lats = []\n",
    "pop_longs = []\n",
    "jobs_lats = []\n",
    "jobs_longs = []\n",
    "for ii,lsoa in enumerate(local_lsoa.index):\n",
    "    oa_list = local_lsoa['oa_list'].loc[lsoa]\n",
    "    tmp = []\n",
    "    tmp_jobs = []\n",
    "    tmp_lat = []\n",
    "    tmp_long = []\n",
    "    for oa in oa_list:\n",
    "        tmp.append(local_oa_population[oa])\n",
    "        tmp_jobs.append(local_oa_number_of_jobs[oa])\n",
    "        tmp_lat.append(local_oa['lat'].loc[oa])\n",
    "        tmp_long.append(local_oa['long'].loc[oa])\n",
    "    # transform into numpy array and normalise into proportions (so that it sums to 1)\n",
    "    tmp = np.array(tmp)/sum(tmp)\n",
    "    tmp_jobs = np.array(tmp_jobs)/sum(tmp_jobs)\n",
    "    tmp_lat = np.array(tmp_lat)\n",
    "    tmp_long = np.array(tmp_long)\n",
    "    pop_lats = np.around(np.sum(tmp_lat * tmp), decimals = 5)\n",
    "    pop_longs = np.around(np.sum(tmp_long * tmp), decimals = 5)\n",
    "    jobs_lats = np.around(np.sum(tmp_lat * tmp_jobs), decimals = 5)\n",
    "    jobs_longs = np.around(np.sum(tmp_long * tmp_jobs), decimals = 5)\n",
    "\n",
    "# now add the columns to the main dataframe\n",
    "local_lsoa_aug0 = local_lsoa.join(pd.DataFrame({'pop_lat': pop_lats, 'pop_long': pop_longs,\n",
    "                                                     'jobs_lat': jobs_lats, 'jobs_long': jobs_longs},\n",
    "                                                 index = local_lsoa.index, \n",
    "                                                    columns = ['pop_lat', 'pop_long','jobs_lat','jobs_long']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 lat      long       ttwa  \\\n",
      "lsoa11                                      \n",
      "E01022100  51.897445 -2.063414  E30000189   \n",
      "E01022101  51.901078 -2.063204  E30000189   \n",
      "E01022102  51.899732 -2.070377  E30000189   \n",
      "\n",
      "                                                     oa_list   pop_lat  \\\n",
      "lsoa11                                                                   \n",
      "E01022100  [E00112298, E00112300, E00112301, E00112306, E...  51.98219   \n",
      "E01022101  [E00112295, E00112296, E00112297, E00112299, E...  51.98219   \n",
      "E01022102  [E00112302, E00112303, E00112304, E00112305, E...  51.98219   \n",
      "\n",
      "           pop_long  jobs_lat  jobs_long  number_of_jobs  density_of_jobs  \\\n",
      "lsoa11                                                                      \n",
      "E01022100   -2.1401  51.98221   -2.14011          1885.0       314.166667   \n",
      "E01022101   -2.1401  51.98221   -2.14011          2500.0       357.142857   \n",
      "E01022102   -2.1401  51.98221   -2.14011          3512.0       439.000000   \n",
      "\n",
      "           max_of_jobs  \n",
      "lsoa11                  \n",
      "E01022100        488.0  \n",
      "E01022101        488.0  \n",
      "E01022102        521.0  \n"
     ]
    }
   ],
   "source": [
    "# sum the number of jobs across all OAs in the same LSOA\n",
    "local_lsoa_number_of_jobs = []\n",
    "local_lsoa_density_of_jobs = []\n",
    "local_lsoa_max_of_jobs = []\n",
    "for ii,lsoa in enumerate(local_lsoa.index):\n",
    "    oa_list = local_lsoa['oa_list'].loc[lsoa]\n",
    "    tot_lsoa_jobs = []\n",
    "    for oa in oa_list:\n",
    "        tot_lsoa_jobs.append(local_oa_number_of_jobs[oa])\n",
    "    # add the absolute number of jobs\n",
    "    local_lsoa_number_of_jobs.append(sum(tot_lsoa_jobs))\n",
    "    local_lsoa_density_of_jobs.append(np.mean(tot_lsoa_jobs))\n",
    "    local_lsoa_max_of_jobs.append(max(tot_lsoa_jobs))\n",
    "# augment the dataframe with the total number of jobs\n",
    "local_lsoa_aug1 = local_lsoa_aug0.join(pd.DataFrame({'number_of_jobs': local_lsoa_number_of_jobs, \n",
    "                                                 'density_of_jobs': local_lsoa_density_of_jobs,\n",
    "                                                 'max_of_jobs': local_lsoa_max_of_jobs},\n",
    "                                                 index = local_lsoa.index, \n",
    "                                                    columns = ['number_of_jobs','density_of_jobs','max_of_jobs']))\n",
    "\n",
    "print(local_lsoa_aug1.head(n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'local_lsoa_aug' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-07f3f4317acc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# order the LSOAs by number of jobs, in descending order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#local_lsoa_ordered2 = local_lsoa_aug.sort_values('number_of_jobs', ascending = False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlocal_lsoa_ordered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_lsoa_aug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'number_of_jobs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# plot the number of jobs, just to see if there is a knee somewhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'local_lsoa_aug' is not defined"
     ]
    }
   ],
   "source": [
    "# order the LSOAs by number of jobs, in descending order\n",
    "#local_lsoa_ordered2 = local_lsoa_aug.sort_values('number_of_jobs', ascending = False)\n",
    "local_lsoa_ordered = local_lsoa_aug.sort_values('number_of_jobs', ascending = False)\n",
    "\n",
    "# plot the number of jobs, just to see if there is a knee somewhere\n",
    "plt.figure(figsize = (20,5))\n",
    "tmp = plt.plot(local_lsoa_ordered['number_of_jobs'][0:80],'-x')\n",
    "tmp = plt.plot(local_lsoa_ordered['density_of_jobs'][0:80],'-o')\n",
    "tmp = plt.plot(local_lsoa_ordered['max_of_jobs'][0:80],'-s')\n",
    "plt.xlabel('LSOA' , fontsize = 12)\n",
    "plt.ylabel('Jobs', fontsize = 12)\n",
    "tmp =plt.xticks(local_lsoa_ordered.index[0:80], rotation = 'vertical', size = 11)\n",
    "ax = plt.gca()\n",
    "for item in (ax.get_yticklabels()):\n",
    "    item.set_fontsize(11)\n",
    "plt.legend(['Absolute job number', 'job density', 'max jobs in one OA',], \n",
    "           fontsize = 12)\n",
    "#plt.ylabel()\n",
    "\n",
    "plt.savefig(folder3 + 'PIN/{}_jobs_per_LSOA.svg'.format(ttwa_name))\n",
    "\n",
    "# save the list, with the number of jobs, and the data collected before (jobs number and breakdown per OA)\n",
    "local_oa_path = folder4 + '{}_oa_lsoa_jobs.pickle'.format(ttwa_name)\n",
    "SAVE_LSOA_DATA = True\n",
    "data_missing = not os.path.isfile(local_oa_path)\n",
    "if SAVE_LSOA_DATA and CALL_LMI and data_missing:\n",
    "    with open(local_oa_path, 'wb') as f:\n",
    "        pickle.dump((local_lsoa_ordered,local_oa_number_of_jobs,local_jobs_socGroups,\n",
    "                     local_oa_jobs_breakdown,local_oa_population),f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['E01022123', 'E01022147', 'E01022107', 'E01022106', 'E01022432',\n",
       "       'E01022116', 'E01022436', 'E01022454', 'E01022435', 'E01022420'],\n",
       "      dtype='object', name='lsoa11')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the 10 OAs with the most jobs\n",
    "origin_lsoas= list(local_lsoa_ordered.index)\n",
    "N = int(np.ceil(len(origin_lsoas) / 10))\n",
    "\n",
    "dest_lsoas = list(local_lsoa_ordered.index[0:N])\n",
    "print('Destination LSOAs: ')\n",
    "dest_lsoas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://transportapi.com/v3/uk/public/journey/from/lonlat:{:5f},{:5f}/to/lonlat:{:5f},{:5f}/at/2019-05-29/07:30.json?app_id=87edbe5c&app_key=6d207ab55f2768d85de4124b5fc4844c\n"
     ]
    }
   ],
   "source": [
    "# from here on, it's about the travel time matrix\n",
    "# define my app_key and app_id\n",
    "app_key = '6d207ab55f2768d85de4124b5fc4844c'\n",
    "app_id = '87edbe5c'\n",
    "app_key_jyl = '09c50d6b59698d5cbe85b50ee758baf6'\n",
    "app_id_jyl = 'c99a83a0'\n",
    "lon_from= '{}'\n",
    "lat_from = '{}'\n",
    "lon_to = '{}'\n",
    "lat_to = '{}'\n",
    "dep_date = '2019-05-29' #set the departure date to the next working day\n",
    "dep_time = '07:30' # set the departure time to 7am\n",
    "# this is the base urlname to call\n",
    "MY_KEY = True\n",
    "urlname_var = 'https://transportapi.com/v3/uk/public/journey/from/lonlat:{:5f},{:5f}/to/lonlat:{:5f},{:5f}/'\n",
    "if MY_KEY:\n",
    "    urlname_fix = 'at/{}/{}.json?app_id={}&app_key={}'.format(\n",
    "        dep_date,dep_time,app_id,app_key)\n",
    "else:\n",
    "    urlname_fix = 'at/{}/{}.json?app_id={}&app_key={}'.format(\n",
    "        dep_date,dep_time,app_id_jyl,app_key_jyl)\n",
    "\n",
    "print(urlname_var + urlname_fix)\n",
    "nb_of_calls = 0\n",
    "max_calls = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute commute times for all the destinations in this TTWA\n",
    "# CAREFUL: This will call the TransportAPI\n",
    "API = True\n",
    "if API:\n",
    "    # save in a folder named after the TTWA we are doing\n",
    "    dest_folder = folder3 + 'PIN/{}'.format(ttwa_formal_name)\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "    \n",
    "    for lsoa_dest in dest_lsoas:\n",
    "        save_path_dest1 = dest_folder + '/{}_all_lsoa_commute_{}.pickle'.format(ttwa_name,lsoa_dest)\n",
    "        all_commute_times = {lsoa_destination1: {}}\n",
    "        for ii,lsoa in enumerate(local_lsoa_ordered.index):\n",
    "            # the API returns a json object, reading and decoding the json object returns a dict\n",
    "            urlname = urlname_var.format(local_lsoa_ordered['long'].loc[lsoa],\n",
    "                                     local_lsoa_ordered['lat'].loc[lsoa],\n",
    "                                     local_lsoa_ordered['long'].loc[lsoa_dest],\n",
    "                                     local_lsoa_ordered['lat'].loc[lsoa_dest]) + urlname_fix\n",
    "            out = requests.get(urlname).json()\n",
    "            nb_of_calls += 1\n",
    "            all_commute_times[lsoa_destination1][lsoa] = out\n",
    "            # save at every iteration\n",
    "            with open(save_path_dest1, 'wb') as f:\n",
    "                pickle.dump(all_commute_times, f)\n",
    "            print('Got to iteration {} for origin LSOA {}'.format(ii,lsoa))\n",
    "            if ii==max_calls:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LSOA to process for destination LSOA E01022454 is 15\n",
      "Got to iteration 0 for destination LSOA E01022454 and origin LSOA E01032348\n",
      "Got to iteration 1 for destination LSOA E01022454 and origin LSOA E01022128\n",
      "Got to iteration 2 for destination LSOA E01022454 and origin LSOA E01022110\n",
      "Got to iteration 3 for destination LSOA E01022454 and origin LSOA E01022145\n",
      "Got to iteration 4 for destination LSOA E01022454 and origin LSOA E01022465\n",
      "Got to iteration 5 for destination LSOA E01022454 and origin LSOA E01022168\n",
      "Got to iteration 6 for destination LSOA E01022454 and origin LSOA E01022153\n",
      "Got to iteration 7 for destination LSOA E01022454 and origin LSOA E01022121\n",
      "Got to iteration 8 for destination LSOA E01022454 and origin LSOA E01022141\n",
      "Got to iteration 9 for destination LSOA E01022454 and origin LSOA E01022437\n",
      "Got to iteration 10 for destination LSOA E01022454 and origin LSOA E01022167\n",
      "Got to iteration 11 for destination LSOA E01022454 and origin LSOA E01022461\n",
      "Got to iteration 12 for destination LSOA E01022454 and origin LSOA E01032939\n",
      "Got to iteration 13 for destination LSOA E01022454 and origin LSOA E01022140\n",
      "Got to iteration 14 for destination LSOA E01022454 and origin LSOA E01022166\n",
      "Number of LSOA to process for destination LSOA E01022435 is 15\n",
      "Got to iteration 0 for destination LSOA E01022435 and origin LSOA E01032348\n",
      "Got to iteration 1 for destination LSOA E01022435 and origin LSOA E01022128\n",
      "Got to iteration 2 for destination LSOA E01022435 and origin LSOA E01022110\n",
      "Got to iteration 3 for destination LSOA E01022435 and origin LSOA E01022145\n",
      "Got to iteration 4 for destination LSOA E01022435 and origin LSOA E01022465\n",
      "Got to iteration 5 for destination LSOA E01022435 and origin LSOA E01022168\n",
      "Got to iteration 6 for destination LSOA E01022435 and origin LSOA E01022153\n",
      "Got to iteration 7 for destination LSOA E01022435 and origin LSOA E01022121\n",
      "Got to iteration 8 for destination LSOA E01022435 and origin LSOA E01022141\n",
      "Got to iteration 9 for destination LSOA E01022435 and origin LSOA E01022437\n",
      "Got to iteration 10 for destination LSOA E01022435 and origin LSOA E01022167\n",
      "Got to iteration 11 for destination LSOA E01022435 and origin LSOA E01022461\n",
      "Got to iteration 12 for destination LSOA E01022435 and origin LSOA E01032939\n",
      "Got to iteration 13 for destination LSOA E01022435 and origin LSOA E01022140\n",
      "Got to iteration 14 for destination LSOA E01022435 and origin LSOA E01022166\n",
      "Number of LSOA to process for destination LSOA E01022420 is 15\n",
      "Got to iteration 0 for destination LSOA E01022420 and origin LSOA E01032348\n",
      "Got to iteration 1 for destination LSOA E01022420 and origin LSOA E01022128\n",
      "Got to iteration 2 for destination LSOA E01022420 and origin LSOA E01022110\n",
      "Got to iteration 3 for destination LSOA E01022420 and origin LSOA E01022145\n",
      "Got to iteration 4 for destination LSOA E01022420 and origin LSOA E01022465\n",
      "Got to iteration 5 for destination LSOA E01022420 and origin LSOA E01022168\n",
      "Got to iteration 6 for destination LSOA E01022420 and origin LSOA E01022153\n",
      "Got to iteration 7 for destination LSOA E01022420 and origin LSOA E01022121\n",
      "Got to iteration 8 for destination LSOA E01022420 and origin LSOA E01022141\n",
      "Got to iteration 9 for destination LSOA E01022420 and origin LSOA E01022437\n",
      "Got to iteration 10 for destination LSOA E01022420 and origin LSOA E01022167\n",
      "Got to iteration 11 for destination LSOA E01022420 and origin LSOA E01022461\n",
      "Got to iteration 12 for destination LSOA E01022420 and origin LSOA E01032939\n",
      "Got to iteration 13 for destination LSOA E01022420 and origin LSOA E01022140\n",
      "Got to iteration 14 for destination LSOA E01022420 and origin LSOA E01022166\n"
     ]
    }
   ],
   "source": [
    "# this version is to fill in missing data in already existing files\n",
    "API = True\n",
    "COMPLETE = False\n",
    "if API and COMPLETE:\n",
    "    # load all saved data, check what's missing\n",
    "    version_load = ''\n",
    "    version_save = '_v2'\n",
    "    for dest_lsoa in dest_lsoas:\n",
    "        lsoa_commute_file = folder3 + 'PIN/{}/{}_all_lsoa_commute_{}{}.pickle'.format(ttwa_formal_name,\n",
    "                                                                                      ttwa_name,dest_lsoa,\n",
    "                                                                                       version_load)\n",
    "        lsoa_commute_file_new = folder3 + 'PIN/{}/{}_all_lsoa_commute_{}{}.pickle'.format(ttwa_formal_name,\n",
    "                                                                                      ttwa_name,dest_lsoa,\n",
    "                                                                                       version_save)\n",
    "        with open(lsoa_commute_file, 'rb') as f:\n",
    "            lsoa_commute = pickle.load(f)\n",
    "\n",
    "        # get the missing origin lsoas\n",
    "        missing_lsoas = []\n",
    "        for ii,lsoa in enumerate(origin_lsoas):\n",
    "            lsoanotin = not lsoa in lsoa_commute[dest_lsoa]\n",
    "            if not lsoanotin:\n",
    "                routesnotin = not 'routes' in lsoa_commute[dest_lsoa][lsoa]\n",
    "            else:\n",
    "                routesnotin = False\n",
    "            if lsoanotin or routesnotin:\n",
    "                missing_lsoas.append(lsoa)\n",
    "        flag_api = len(missing_lsoas) and nb_of_calls<max_calls #(len(missing_lsoas)+nb_of_calls)<= max_calls\n",
    "        #print(missing_lsoas, flag_api)\n",
    "        #continue\n",
    "        if flag_api:\n",
    "            print('Number of LSOA to process for destination LSOA {} is {}'.format(dest_lsoa,len(missing_lsoas)))\n",
    "            #all_commute_times = {lsoa_destination1: {}}\n",
    "            for ii,lsoa in enumerate(missing_lsoas):\n",
    "                # the API returns a json object, reading and decoding the json object returns a dict\n",
    "                urlname = urlname_var.format(local_lsoa_ordered['long'].loc[lsoa],\n",
    "                                         local_lsoa_ordered['lat'].loc[lsoa],\n",
    "                                         local_lsoa_ordered['long'].loc[dest_lsoa],\n",
    "                                         local_lsoa_ordered['lat'].loc[dest_lsoa]) + urlname_fix\n",
    "                out = requests.get(urlname).json()\n",
    "                nb_of_calls += 1\n",
    "                lsoa_commute[dest_lsoa][lsoa] = out\n",
    "                # save at every iteration\n",
    "                with open(lsoa_commute_file_new, 'wb') as f:\n",
    "                    pickle.dump(lsoa_commute, f)\n",
    "                print('Got to iteration {} for destination LSOA {} and origin LSOA {}'.format(ii,dest_lsoa,lsoa))\n",
    "                if nb_of_calls==max_calls:\n",
    "                    break\n",
    "    # for cheltenham I'm missing 144 pairs in total: can collect from the first 7 today (99 calls in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Maps API Key:\n",
    "AIzaSyD1c5PVpgYpSh4TMLCrZ51lAiWTh3Bdg-M\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
